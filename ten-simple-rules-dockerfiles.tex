% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}


\usepackage[user,titleref]{zref}
\newcommand*{\rulelabel}[2]{\ztitlerefsetup{title=#1} \zlabel{#2} \label{#2} \zrefused{#2}}
\newcommand*{\ruleref}[1]{\hyperref[{#1}]{Rule~\ztitleref{#1}}}



\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Writing Dockerfiles for Reproducible Research} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Daniel Nüst\textsuperscript{\getAff{Institute for Geoinformatics, University of Muenster, Muenster, Germany}}\textsuperscript{*},
Vanessa Sochat\textsuperscript{\getAff{Stanford Research Computing Center, Stanford University, Stanford, CA,
US}},
Ben Marwick\textsuperscript{\getAff{Department of Anthropology, University of Washington, Seattle, USA}},
Stephen Eglen\textsuperscript{\getAff{Department of Applied Mathematics and Theoretical Physics, University of
Cambridge, Cambridge, Cambridgeshire, GB}},
Tim Head\textsuperscript{\getAff{Wild Tree Tech, Zurich, CH}},
Tony Hirst\textsuperscript{\getAff{Department of Computing and Communications, The Open University, GB}}\\
\bigskip
\bigskip
* Corresponding author: daniel.nuest@uni-muenster.de\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Containers are greatly improving computational science by packaging
software and data dependencies. In a scholarly context, transparency and
support of reproducibility are the largest drivers for using these
containers. It follows that choices that are made with respect to
building containers can make or break a workflow's reproducibility. The
build for the container image is often created based on the instructions
in a plain-text file. For example, one such container technology,
Docker, provides instructions using a \texttt{Dockerfile}. By following
the rules in this article researchers writing a \texttt{Dockerfile} can
effectively create containers suitable for sharing with fellow
scientists, for inclusing in scholarly communication such as education
or scientific papers, and for an effective and sustainable personal
workflow.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

With access to version control systems (VCS, {[}1{]}) and collaboration
platforms based on these, such as \href{https://github.com}{GitHub} or
\href{https://gitlab.com}{GitLab}, it has become increasingly easy to
not only share abstract algorithms and study designs, but also
instructions for executing whole research workflows, including building
and testing of the software used. The publication of these instructions
and related configuration files are a response to the increasing
complexity of computer-based research, which is too complicated for
``papers'' based on the traditional journal article format to fully
communicate the details of research {[}2{]}, but where the actual
contribution to knowledge includes the full software environment that
produced a result {[}3{]}. A \emph{research compendium} is a compilation
of data, code, and documentation that accompanies, or is itself, a
scholarly publication for increased transparency and reproducibility
(cf.~\url{https://research-compendium.science}). Within such a
compendium, it is desirable to include instructions, i.e.~a human-
\emph{and} machine-readable recipe, for building containers that capture
the computing environment, i.e., all software and data dependencies. By
providing this recipe, authors of scientific articles greatly improve
their work's level of documentation, transparency, and reusability. Such
practice is one important part of common practices for scientific
computing {[}4,5{]}, with the result that it is much more likely both
the author and others are able to reproduce and extend an analysis
workflow. The containers built from these recipes are portable
encapsulated snapshots of a specific computing environment. Such
containers have been demonstrated for capturing scientific notebooks
{[}6{]} and reproducible workflows {[}7{]}.

While there are several tutorials for using containers for reproducible
research {[}8--12{]}, there is no detailed manual for how to write the
actual instructions to create the containers beyond generic best
practices outside of the scientific domain {[}13{]}. Several platforms
for facilitating reproducible research are built on top of containers
{[}14--18{]}, but they hide most of the complexity from the researcher.
Because \emph{``the number of unique research environments approximates
the number of researchers''} {[}18{]}, sticking to conventions helps
every researcher to understand, modify, and eventually write container
recipes suitable for their needs. Even if they are not sure how the
technology behind them actually works, reserachers can leverage
containerisation, but should craft their own definition of computing
environments following good practices. The practices outlined here are
strongly related to software engineering in general and research
software engineering (RSEng) in particlar, which is concerned with
quality, training, and recognition of software in science {[}19{]}.
Research Software Engineers (RSEs) are not the target audience for this
work, but we want to encourage you to reach out to your local or
national RSE community if your needs go beyond the contents detailed
here.

While there are many different container technologies, this article
focuses on Docker {[}20{]}. Docker is a highly suitable tool for
reproducible research (e.g., {[}21{]}) and our observations indicate it
is the most widely used container technology in academic data science.
The goal of this article is to guide you to write a \texttt{Dockerfile}
so that it best facilitates interactive development and computer-based
research, as well as the higher goals of reproducibility and
preservation of knowledge. Such practices are generally not part of
generic containerization tutorials and are rarely found in published
\texttt{Dockerfile}s, which are often used as templates by novices. The
differences between a helpful, stable \texttt{Dockerfile} and one that
is misleading, prone to failure, and full of potential obstacles, are
not obvious, especially for researchers who do not have extensive
software development experience or formal training. A commitment to this
article's rules can ensure that workflows are reproducible and reusable,
that computing environments are understandable by others, and
researchers can collaborate effectively. Their application should not be
triggered by the publication of a finished project, but be weaved into
day-to-day habits (cf.~thoughts on openness as an afterthought by
{[}22{]} and on computational reproducibility by {[}3{]}).

To start with, we assume you have a scripted scientific workflow,
i.e.~you can, at least at a certain point in time, execute the full
process with a single command, for example \texttt{make\ my\_workflow},
\texttt{Rscript\ analysis.R}, or \texttt{python3\ paper-plots.py}. This
execution should be able to be triggered by command-line instruction,
which is possible for all generic programming languages and widely used
workflow tools, but may also be opening and starting a process with a
graphical user interface. A workflow that does not support scripted
execution is out of scope for reproducible research, and does not fit
well with containerization.

\hypertarget{docker}{%
\section{Docker}\label{docker}}

Docker is a container technology that is widely adopted and supported on
many platforms, and has become highly useful in science. They are
distinct from virtual machines (VM) or hypervisors as they do not
emulate hardware or operating system kernels, and thus do not require
the same system resources. To create Docker containers, we write text
files that follow a particular format called \texttt{Dockerfile}
{[}23{]}. \texttt{Dockerfile}s are machine- and human-readable recipes
for building images. Images are inert, immutable, read-only files that
include the application, e.g., the programming language interpreter
needed to run a workflow, and the environment required by the
application to run. A \texttt{Dockerfile} consists of a sequence of
instructions to copy files and install software, which add layers to the
image. These layers can be used for caching across image builds, which
is important for minimizing build and download times. The images have a
main executable that is started when they are run as stateful
containers, which are the running instances of Docker images. Containers
can be modified, stopped, restarted and purged.

While Docker was the original technology to support this format, other
container technologies support the format including
\href{https://podman.io/}{podman}/\href{https://github.com/containers/buildah}{buildah}
supported by RedHat,
\href{https://github.com/GoogleContainerTools/kaniko}{kaniko},
\href{https://github.com/genuinetools/img}{img}, and
\href{https://github.com/moby/buildkit}{buildkit}. The Singularity
container software {[}24{]} is optimized for high performance computing
and although it uses its own format, the \emph{Singularity recipe}, it
can import Docker images directly from a Docker registry. Although the
Singularity recipe format is different, the rules here are transferable
to some extent. While some may argue for reasons to not publish
reproducibly, e.g., lack of time and incentives, reluctance to share
(cf.~{[}25{]}) and there are substantial technical challenges to
maintain software and documentation, providing a \texttt{Dockerfile}, a
pre-built Docker image, or other type of container should become an
increasingly easier task for the average researcher. If a researcher is
able to find and create containers or write a \texttt{Dockerfile} to
address their most common use cases, then arguably it will not be extra
work after this initial set up (cf.~README of {[}26{]}). In fact, the
\texttt{Dockerfile} itself can be a powerful documentation to show from
where data and code was derived, i.e.~downloaded or installed, and
consequently where a third party might obtain them again.

\hypertarget{consider-tools-to-assist-with-dockerfile-generation}{%
\section*{1. Consider tools to assist with Dockerfile
generation}\label{consider-tools-to-assist-with-dockerfile-generation}}
\addcontentsline{toc}{section}{1. Consider tools to assist with
Dockerfile generation}

\ztitlerefsetup{title=1} \zlabel{rule:tools} \label{rule:tools} \zrefused{rule:tools}

Writing a \texttt{Dockerfile} from scratch is not that simple, and even
experts sometimes take shortcuts. Thus, it is a good strategy to first
look to tools that can help to generate a \texttt{Dockerfile} for you.
Such tools have likely thought about and implemented good practices, and
they may have added newer practices when reapplied at a later point in
time. Therefore the most important rule is to apply a multi-step process
for your specific use case. First, you want to determine if there is an
already existing container that you can use, and in this case, use it
and add to your workflow documentation instructions for doing so. As an
example, you might be doing some kind of interactive development. For
interactive development environments such as notebooks and development
servers or databases, you can readily find containers that come
installed with all the software that you need. Second, in the case that
there isn't an existing container for your needs, you might next look to
well-maintained tools to help with \texttt{Dockerfile} generation. These
tools can add required software packages to an existing image without
you having to manually write a \texttt{Dockerfile} at all.
Well-maintained includes the tool's own stability and usability, but
also that suitable base images are used, likely from the official Docker
library {[}27{]}, to ensure that the container has the most recent
security fixes for the operating system in question. Third, you may want
to write you own \texttt{Dockerfile} from scratch if these tools do not
suffice for your needs. \emph{In this case, follow the remaining rules.}

\hypertarget{tools-for-container-generation}{%
\subsection{Tools for container
generation}\label{tools-for-container-generation}}

Repo2docker {[}17{]} is a tool that is maintained by
\href{https://jupyter.org/}{Project Jupyter} that can help to transform
a repository with common simple configuration files for defining
software dependencies and versioninto a container. As an example, we
might install \texttt{jupyter-repo2docker} and then run it against a
repository with a \texttt{requirements.txt} file, an indication of being
a Python workflow with dependencies on the
\href{https://pypi.org/}{Python Package Index} (PyPI) with the following
command.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{jupyter-repo2docker}\NormalTok{ https://github.com/norvig/pytudes}
\end{Highlighting}
\end{Shaded}

The resulting container image installs the dependencies listed in the
requirements file, along with providing an entrypoint to run a notebook
server to interact with any existing workflows in the repository. A
precaution that needs to be taken is that the default command above will
create a home for the current user, meaning that the container itself
would not be ideal to share, but rather any researchers interested in
interaction with the code inside should build their own container. For
this reason, it is good practice to look at any help command provided by
the tool and check for configuration options for user names, user ids,
and similar. It's also recommended to add custom labels to your
container build to define metadata for your analyses even if you are
using tools
(see~\hyperref[{rule:metadata}]{Rule~\ztitleref{rule:metadata}}).

Additional tools to assist with writing \texttt{Dockerfile}s include
\texttt{containerit} {[}28{]} and \texttt{dockta} {[}29{]}.
\texttt{containerit} automates the generation of standalone
\texttt{Dockerfile}s for workflows in R. It can provide a starting point
for users unfamiliar with writing \texttt{Dockerfile}s, or together with
other R packages provide a full image creation and execution process
without having to leave an R session. \texttt{dockta} supports multiple
programming languages and configurations files, just as
\texttt{repo2docker}, but attempts to create readable
\texttt{Dockerfile}s compatible with plain Docker and to improve user
experience by cleverly adjusting instructions to reduce build time.

\hypertarget{use-versioned-and-automatically-built-base-images}{%
\section*{2. Use versioned and automatically built base
images}\label{use-versioned-and-automatically-built-base-images}}
\addcontentsline{toc}{section}{2. Use versioned and automatically built
base images}

\ztitlerefsetup{title=2} \zlabel{rule:base} \label{rule:base} \zrefused{rule:base}

It's good practice to use base images that are maintained by the Docker
library, so called \emph{``official images''} {[}30{]}, which benefit
from a review for best practices and vulnerability scanning {[}13{]}.
You can identify these images by the missing the user portion of the
image name, which comes before the \texttt{/}, e.g., \texttt{r-base} or
\texttt{python}. There are also repositories that are maintained by
organisations or fellow researchers. While some organizations can be
trusted to update containers with security fixes (e.g., the
\href{https://hub.docker.com/u/rocker-org}{Rocker Project} or
\href{https://hub.docker.com/u/jupyter}{Jupyter}), for most individual
accounts that provide ready to use images, it is likely that these will
not be updated regularly. It is even possible that images or
\texttt{Dockerfile}s may disappear, or that images are published with
malicious intent. Images furthermore have \emph{tags} associated with
them with specific meanings, usually as a version number but sometimes
also variants of images. Tags are defined at image built time and appear
in image name after the \texttt{:} when you use an image, e.g.,
\texttt{python:3.6}. By \emph{convention} a missing tag is assumed to be
the word \texttt{latest}. When you re-build you images locally using the
same tag, the tag is always associated with the newest built. But the
image also has an automatically generated image name using a hash, which
will \emph{not} be updated.´ So if you tag an image, use the tag when
you start a container.

Because of these options, a good understanding of how base images and
image tags work is crucial, as the image and tag that you choose has
important implications for your own image. You should know how to use
\texttt{FROM} instructions to trace all the way up to the original
container base, an abstract base called \texttt{scratch}. Doing this
kind of trace is essential because it makes clear all of the steps that
were taken to generate your container and who took them. If you want to
build on a third party image, carefully consider the author/maintainer
and \emph{never} use an image without a published \texttt{Dockerfile}.
If you want to use an unofficial image, do save a copy of the
\texttt{Dockerfile}s created by others. Alternatively, copy relevant
instructions into your own \texttt{Dockerfile}, acknowledge the source
in a comment, and configure an automated build for your own image. The
official images and images by established organisations should be your
preferred source of snippets if you craft you own \texttt{Dockerfile}
Automated builds can be complex to set up, and details are out of scope
of this article. The documentation for, and toolsets provided by
container registries such as
\href{https://docs.docker.com/docker-hub/builds/}{Docker Hub} or
\href{https://docs.gitlab.com/ee/user/packages/container_registry/index.html\#build-and-push-images}{GitLab}
as well as CI platforms such as
\href{https://github.com/actions/starter-workflows/tree/master/ci}{GitHub
actions} or
\href{https://circleci.com/orbs/registry/orb/circleci/docker\#commands-build}{CircleCI}
can help you get started; tools may also be available to automate builds
using intermediate tools, such as the \texttt{repo2docker} Github action
{[}31,32{]}, You should avoid publishing an image in a public registry
with \texttt{docker\ push}, because it is opaquely breaking the linkage
between \texttt{Dockerfile} and image.

A tag like \texttt{latest} is good in that security fixes and updates
are likely present, however it is bad in that it is a moving target so
that it is more likely that an updated base could break your workflow.
Other tags that should be avoided are \texttt{dev}, \texttt{devel}, or
\texttt{nightly} that may provide possibly unstable versions of the
software. Official builds are often tagged with corresponding release
version numbers, the newest of which also has the \texttt{latest} tag.
For example, if you build a container with \texttt{python:latest} that
only supports Python version 3, when the base is updated to Python 4
it's likely that your software won't work as expected. Therefore a tag
like \texttt{python:3.5} is a preferable choice, where the lack of the
third component of the version, i.e., \texttt{3.5.x}, ensures that
security patches and bugfixes \emph{will} be included. These
\emph{bugfix} version changes should not break your code, i.e., are
backwards compatible (cf.~semantic versioning, {[}33{]}), and if they
break your code you might rely on a bug in the used software. When you
choose a base image, try to choose one with a Linux distribution that
supports the software stack you are using, and also take into account
the bases that are widely used by your community. As an example, Ubuntu
is heavily used for geospatial research, and so the
\texttt{rocker/geospatial} image would be a good choice for spatial data
science with R, or \texttt{jupyter/tensorflow-notebook} could be a good
choice for machine learning with Python.

Container tags may also provide an indication as the size of a
container. Smaller images are often indicated by having \texttt{slim} or
\texttt{minimal} as part of the tag. If you need to build smaller
containers, consider a \texttt{busybox} base or a \emph{multi-stage
build} {[}34{]}, which allows you to selectively copy files from one
build step to another. Also take into account the software that you
actually need, but mostly for clarity. In scientific contexts, the image
size will be often the least of you concerns, because data size is much
larger or image portability is not time critical.

Base images that have complex software installed (e.g.~machine learning
tool stacks, specific
\href{https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms}{BLAS}
library) are helpful and fine to use. If you do not want to rely on a
third party or other individual to maintain the recipe and container,
you should copy the \texttt{Dockerfile} to your own repository, and also
provide a deployment for it via your own automated build. Trusting that
the third party \texttt{Dockerfile} will persist for your analyses may
be risky because that \texttt{Dockerfile} could disappear without
warning. Here is a selection of communities that produce widely used
regular builds and updates:

\begin{itemize}
\tightlist
\item
  \href{https://www.rocker-project.org/}{Rocker} for R {[}21{]}
\item
  \href{https://bioconductor.org/help/docker/}{Docker containers for
  Bioconductor} for bioinformatics
\item
  \href{https://hub.docker.com/_/neurodebian}{NeuroDebian images} for
  neuroscience {[}35{]}
\item
  {[}Jupyter Docker
  Stacks{]}(https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html
  for Notebook-based computing
\item
  \href{https://hub.docker.com/r/taverna/taverna-server}{Taverna Server}
  for running Taverna workflows
\end{itemize}

For example, here is how we would use a base image \texttt{r-ver} with
tag \texttt{3.5.2} from the \texttt{rocker} organization on Docker Hub
(\texttt{docker.io}).

\begin{verbatim}
FROM rocker/r-ver:3.5.2
\end{verbatim}

\hypertarget{use-formatting-document-within-and-favor-clarity}{%
\section{3. Use formatting, document within, and favor
clarity}\label{use-formatting-document-within-and-favor-clarity}}

\ztitlerefsetup{title=3} \zlabel{rule:formatting} \label{rule:formatting} \zrefused{rule:formatting}

It is good practice to think of the \texttt{Dockerfile} as a human
\emph{and} machine readable file. This means that you should use
indentation, new lines, and comments to make your \texttt{Dockerfile}s
well documented and readable. Specifically, carefully indent commands
and their arguments to make clear what belongs together, especially when
connecting multiple commands in a \texttt{RUN} instruction with
\texttt{\&\&}. Use \texttt{\textbackslash{}} at the end of a line to
break a single command into multiple lines. This will ensure that no
single line gets too long to comfortably read. Use long versions of
parameters for readability (e.g., \texttt{-\/-input} instead of
\texttt{-i}). When you need to change a directory, use \texttt{WORKDIR},
because it not only creates the directory if it doesn't exist but also
persist the change across multiple \texttt{RUN} instructions.

You can use a linter {[}36{]} to avoid small mistakes and follow good
practices from software development communities. The consistency added
by linting also helps keeping your edits to a \texttt{Dockerfile} in a
VCS meaningful (see
\hyperref[{rule:publish}]{Rule~\ztitleref{rule:publish}}). Note however
that a linter's rules may not primarily serve the intention of
reproducible scientific workflows.

As you are writing the \texttt{Dockerfile}, be mindful of how other
people will read it. Are your choices and commands being executed clear,
or is further comment warranted? To assist others in making sense of
your \texttt{Dockerfile}, you can add comments that include links to
online forums, code repository issues, or VCS commit messages to give
context for your specific decisions, for example
\href{https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile}{this
\texttt{Dockerfile} by Kaggle} does a good job at explainig the
reasoning and steps of the contained instructions. Comments should
include helpful usage guides and links for readers inspecting the
\texttt{Dockerfile}, including future you. Dependencies can be grouped
in this fashion, which also makes it easier to spot changes if your
\texttt{Dockerfile} is managed in a VCS (see
\hyperref[{rule:publish}]{Rule~\ztitleref{rule:publish}}). It can even
be helpful to include comments about commands that did not work so you
do not fall repeat past mistakes. If you find that you need to remember
an undocumented step, that is an indication that it should be documented
in the \texttt{Dockerfile}.

Labels are useful to provide a more structured form of documentation
about software, especially for users of supporting infrastructure that
might not expose the actual \texttt{Dockerfile} (see
\hyperref[{rule:metadata}]{Rule~\ztitleref{rule:metadata}})¸. It is
often helpful to provide commented lines with \texttt{docker\ build} and
\texttt{docker\ run} within the \texttt{Dockerfile} to show how to build
and run the image, even though these lines are not necessary for a valid
\texttt{Dockerfile}, this is a convenient place to record them. These
comments can be especially relevant if volume mounts or ports are
important for using the container, and by putting them at the end of the
file, they are more likely to be seen, easy to copy-paste after a
container build, and to be in consistent state compared to documentation
in another file. Here is an example of a commented section to show build
and usage.

\begin{verbatim}
# Build the images with
##> docker build --tag great_workflow .
# Run the image:
##> docker run --it --port 80:80 --volume ./input:/input --name gwf great_workflow
# Extract the data:
##> docker cp gwf:/output/ ./output
\end{verbatim}

If you were to discover a previously written \texttt{Dockerfile} and not
remember the container identifier you used, it would be represented in
the \texttt{Dockerfile} in the \texttt{-\/-name} parameter, preserved in
a comment. Following a common coding aphorism, we might say \emph{``A
Dockerfile written three months ago may just as well have been written
by someone else''}. Here is a selection of typical kinds of comments
that are useful to include in a \texttt{Dockerfile}:

\begin{verbatim}
# apt-get install specific version, use 'apt-cache madison <pkg>' 
# to see available versions
RUN apt-get install python3-pandas=0.23.3+dfsg-4ubuntu1

# RUN command spreading several lines
RUN R -e 'getOption("repos")' && \
  install2.r \
    fortunes \
    here

# this library must be installed from source to get version newer
# than in sources
RUN git clone http://url.of/repo && \
  cd repo && \
  make build && \
  make install

# following commands from instructions at <LINK HERE>
\end{verbatim}

Clarity is always more important than brevity. For example, if your
container uses a script to run a complex install routine, instead of
removing it from the container upon completion, which is commonly seen
in production \texttt{Dockerfile}s aiming at small image size, you
should keep the script in the container for a future user to inspect.
Depending on the programming language used, your project may already
contain files to manage dependencies and you may use a package manager
to control this aspect of the computing environment. This is a very good
practice and helpful, though you should consider the externalization of
content to outside of the \texttt{Dockerfile} (see
\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}). A single long
\texttt{Dockerfile} with sections and helpful comments can be complete
and thus more understandable than a collection of separate files.

Generally, aim to design the \texttt{RUN} instructions so that each
performs one scoped action, e.g., download, compile, and install one
tool. Each instruction will result in a new layer, and reasonably
grouped changes increase readability of the \texttt{Dockerfile} and
facilitate inspection of the image, e.g., with tools like dive {[}37{]}.
Convoluted \texttt{RUN} instructions can be acceptable to reduce the
number of layers, but careful layout and consistent formatting should be
applied. A \texttt{RUN} instruction longer than a page requires
scrolling, diminishing readability. This may be challenging for the next
reader to digest and you should consider splitting it up, being aware of
the extra layers added to the image.

When you install several system libraries, it is good practice to add
comments about why the dependencies are needed. This way, if a piece of
software is removed from the container, it will be easier to remove the
system dependencies that are no longer needed. If you intend to build
the image more than once (perhaps during development) and you can take
advantage of build caching to avoid execution of time-consuming
instructions, e.g., install from a remote resource or a file that gets
cached. You should list commands \emph{in order} of least likely to
change to most likely to change and use the \texttt{-\/-no-cache} flag
to force a re-build of all layers. Docker will execute the instructions
in the order as they appear in the \texttt{Dockerfile}. When one
instruction is completed, the result is cached, and the build moves to
the next one. If you change something in the Dockerfile, and rebuild the
container, each instruction is inspected in turn. If it hasn't changed,
the cached layer is used and the build progresses. If the line has
changed, that build step is executed afresh and then every following
instruction will have to be rebuilt in case the line that changed
influcences something for a following instruction. A recommended
ordering based on these considerations is as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  system libraries
\item
  language-specific libraries or modules
\item
  from repositories (binaries)
\item
  from source
\item
  own software/scripts (if not mounted)
\item
  labels
\item
  \texttt{RUN}/\texttt{ENTRYPOINT}
\end{enumerate}

Finally, as a supplement to content inside the \texttt{Dockerfile}, it
is good practice to also write a section in a \texttt{README} alongside
the \texttt{Dockerfile} for exactly how to build, run, and otherwise
interact with the container. If a pre-built image is provided on Docker
Hub, you should direct the user to it in your \texttt{README}.

\hypertarget{define-version-numbers-for-reproducible-builds}{%
\section*{4. Define version numbers for reproducible
builds}\label{define-version-numbers-for-reproducible-builds}}
\addcontentsline{toc}{section}{4. Define version numbers for
reproducible builds}

\ztitlerefsetup{title=4} \zlabel{rule:pinning} \label{rule:pinning} \zrefused{rule:pinning}

The reproducibility of your \texttt{Dockerfile} heavily depends on how
well you define the versions of software to be installed in the image.
The more specific, the better, because using the desired version leads
to reproducible builds. The practice of specifying versions of software
is called \emph{version pinning} (e.g., on \texttt{apt}:
https://blog.backslasher.net/my-pinning-guidelines.html). For stable
workflows in a scientific context, it is generally advised to freeze the
computing environment explicitly and not rely on the ``current'' or
``latest'' software, which is a moving target.

\hypertarget{system-libraries}{%
\subsection{System libraries}\label{system-libraries}}

System library versions can largely come from the base image tag that
you choose to use, e.g., \texttt{ubuntu:18.04}, because the operating
system's software repositories are very unlikely to introduce breaking
changes, but predominantly fix errors with newer versions. However, you
can also install specific versions of system packages with the
respective package manager. For example, you might want to demonstrate a
bug, prevent a bug in an updated version, or pin a working version if
you suspect an update could lead to a problem. Generally, system
libraries are more stable than software modules supporting analysis
scripts, but in some cases they can be highly relevant to your workflow.
\emph{Installing from source} is a useful way to install very specific
versions, however it comes at the cost of needing build libraries. Here
are some examples of terminal commands that will list the currently
installed versions of software on your system:

\begin{itemize}
\tightlist
\item
  Debian/Ubuntu: \texttt{dpkg\ -\/-list}
\item
  Alpine: \texttt{apk\ -vv\ info\textbar{}sort}
\item
  CentOS: \texttt{yum\ list\ installed} or \texttt{rpm\ -qa}
\end{itemize}

\hypertarget{extension-packages-and-programming-language-modules}{%
\subsection{Extension packages and programming language
modules}\label{extension-packages-and-programming-language-modules}}

In the case of needing to install packages or dependencies for a
specific language, package managers are a good option. Package managers
generally provide reliable mirrors or endpoints to download software,
many packages are tested before release, and most importantly the
provide access to specific versions. Most package managers have a
command line interface that can be used from \texttt{RUN} commands in
your \texttt{Dockerfile}, along with various flavors of ``freeze''
commands that can output a text file listing all software packages and
versions
(cf.~https://markwoodbridge.com/2017/03/05/jupyter-reproducible-science.html
cited by {[}6{]}) The biggest risk with using package managers with
respect to \texttt{Dockerfile}s is outsourcing configuration. As an
example, here are configuration files supported by commonly used
languages in scientific programming:

\begin{itemize}
\tightlist
\item
  Python: \texttt{requirements.txt} (pip tool, {[}38{]}),
  \texttt{environment.yml} (Conda, {[}39{]})
\item
  R: \texttt{DESCRIPTION} file format {[}40{]} and \texttt{r} (``little
  R'', {[}41{]})
\item
  JavaScript: \texttt{package.json} of \texttt{npm} {[}42{]}
\item
  Julia: \texttt{Project.toml} and \texttt{Manifest.toml} {[}43{]}
\end{itemize}

In some cases (e.g., Conda) the package manager is also able to make
decisions about what versions to install, which is likely to lead to a
non-reproducible build. In all of the above, the user is required to
inspect the file or the build to see what is installed. For this reason,
in the case of having few packages, it is suggested to write the install
steps and versions directly into the \texttt{Dockerfile} (also for
clarity, see
\hyperref[{rule:formatting}]{Rule~\ztitleref{rule:formatting}}). For
example, the \texttt{RUN} instruction here:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{RUN}\NormalTok{ pip install geopy==1.20.0 }\KeywordTok{&&} \KeywordTok{\textbackslash{}}
    \ExtensionTok{pip}\NormalTok{ install uszipcode==0.2.2}
\end{Highlighting}
\end{Shaded}

serves as more clear documentation in a \texttt{Dockerfile} than a
\texttt{requirements.txt} file that lists the same:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{RUN}\NormalTok{ pip install -r requirements.txt}
\end{Highlighting}
\end{Shaded}

This modularisation is a potential risk for understandability and
consistency, which can be mitigated by carefully organizing all these
files in the same version-controlled project. You can also use package
manager to install software from source code \texttt{COPY}ied into the
image (see~\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}). And
finally, you can use many package managers to install software from
source obtained from code management repositories, e.g., installing a
specific tool identified by a GitHub version tag or commit hash. Be
aware of the risk of depending on such repositories, especially if they
are out of your control, although the installation command with a full
URL should allow readers of your \texttt{Dockerfile} to dig deeper if
problems arise and such an installation from source gives you a lot of
freedom (including the one to shoot yourself in the foot). The version
pinning capabilities of these file formats and package managers are
described in their respective documentation.

As a final note on software installation, you should be aware of the
\href{https://docs.docker.com/engine/reference/builder/\#user}{\texttt{USER}
instruction} in a \texttt{Dockerfile}. It is common to install all
software as the default user \texttt{root}. In fact it is
\emph{required} for system dependencies. You can switch the user running
subsequent \texttt{RUN}, \texttt{CMD} and \texttt{ENTRYPOINT}
instructions with the \texttt{USER} instruction, but you have to make
sure yourself that the user exists within the image. This is useful if a
software aborts installations as the \texttt{root} user, or to avoid
permission problems when using the container
(see~\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}). Be sure to
document a non-default user in detail and to examine the used users in
base images if you encounter errors on user permissions.

\hypertarget{add-user-scripts-and-data-into-containers-by-mounting-directories}{%
\section*{5. Add user scripts and data into containers by mounting
directories}\label{add-user-scripts-and-data-into-containers-by-mounting-directories}}
\addcontentsline{toc}{section}{5. Add user scripts and data into
containers by mounting directories}

\ztitlerefsetup{title=5} \zlabel{rule:mount} \label{rule:mount} \zrefused{rule:mount}

The role of containers is to provide the methods, not to encapsulate
datasets. It is better to insert data files and scripts files from the
local machine into the container at run time, and using the container
image primarily for the software and dependencies. This insertion is
achieved by using
\href{https://docs.docker.com/storage/bind-mounts/}{\emph{bind mounts}}.
Mounting these files is preferable to using the
\texttt{ADD}/\texttt{COPY} instructions in the \texttt{Dockerfile},
because files persist when the container instance or image is removed
from your system, and the files are more accessible to other users if
published in an online research compendium outside of the image. You
should use \texttt{COPY} if you actually copy files because it is
explicit. \texttt{ADD} supports additional features and you should only
use it if you leverage them: (i) a URL as a source, for which you may
use it instead of a \texttt{RUN} instruction to download files, and (ii)
a local \href{https://en.wikipedia.org/wiki/Tar_(computing)}{tar file},
which you should avoid because such an archive file will be harder for
future readers of your \texttt{Dockerfile} to make sense of. Unless
specific features are needed, bind mounts are also preferable to
\href{https://docs.docker.com/storage/volumes/}{storage volumes}.

Standalone \emph{script files} are distinct from other software as they
are not packaged and versioned, and are treated as files, not as managed
software. If you developed software for a specific analysis in form of a
software package, you should publish in a public source code or software
repository and follow
\hyperref[{rule:pinning}]{Rule~\ztitleref{rule:pinning}} for installing
it. You should avoid installing software packages from source after
\texttt{COPY}ing the code into the image, because the connection between
the file outside of the image and the one copied in is easily lost
(cf.~\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}). Consider
using the suitable package system with pinned versions even for small
scripts if the functions are reusable across datasets or workflows by
you or others. If you cannot publish a software project albeit it being
in a language's package structure, you should make sure the files are
always published together with the \texttt{Dockerfile}
(see~\hyperref[{rule:publish}]{Rule~\ztitleref{rule:publish}}).

Storing \emph{data files} outside of the container further allows
handling of very large datasets and datasets with data worthy of
protection, e.g., proprietary data or private information. Do not
include such data in an image. To avoid publishing sensitive data by
accident, you can add the data directory to the
\href{https://docs.docker.com/engine/reference/commandline/build/\#use-a-dockerignore-file}{\texttt{.dockerignore}}
file, which excludes files and directories from the
\href{https://docs.docker.com/engine/reference/commandline/build/\#extended-description}{build
context}, i.e., the set of files considered by \texttt{docker\ build}.
Ignoring data files also speeds up the build in case of very large files
or many small files, the latter of which makes it common to ignore for
example the \texttt{.git} folder of the Git VCS. You should include
dummy or test data into the image to be able to ensure that a container
is functional without a larger custom dataset, e.g., for automated tests
or instructions in the user manual. For all these cases you should
provide clear instructions for users in the \texttt{README} how to
obtain actual or dummy data. When publishing your workspace, e.g., on
Zenodo, having data and script contents as regular files outside of the
container makes them more accessible to others, for example for reuse or
analysis.

A mount can can also be used to access \emph{output data} from a
container. This can be an extra mount, or the same \texttt{data}
directory. Alternatively, you can use the
\href{https://docs.docker.com/engine/reference/commandline/cp/}{\texttt{docker\ cp}}
command to access files from a running or stopped container, but this
requires a specific handling, e.g., naming the container when starting
it or using multiple shells, that require very detailed instructions for
users.

You can use the \texttt{-v}/\texttt{-\/-volume} or \texttt{-\/-mount}
flags to \texttt{docker\ run} to configure bind mounts of directories or
files {[}44{]}, including options, as shown in the following examples.
If the target path exists within the image, the bind mount will replace
it for the started container.

\begin{verbatim}
# mount directory
docker run --volume /home/user/project:/project mycontainer

# mount directory as read-only
docker run --volume /home/user/project:/project:ro mycontainer

# mount multple directories, one with write access relative to current path (Linux)
docker run --volume /homse/user/Download/article-x-supplement/data:/data:ro --volume $(pwd)/outputs:/output-data:rw mycontainer
\end{verbatim}

How your container expects external resources to be mounted into the
container should be included in the example commands (see
\hyperref[{rule:formatting}]{Rule~\ztitleref{rule:formatting}}). In
these commands you can also make sure to avoid issues with file
permissions by using Docker's \texttt{-\/-user} option. For example, by
default, writing a new file from inside the container will be owned by
user \texttt{root} on your host, because that is the default user within
the container.

\hypertarget{capture-structured-environment-metadata}{%
\section*{6. Capture structured environment
metadata}\label{capture-structured-environment-metadata}}
\addcontentsline{toc}{section}{6. Capture structured environment
metadata}

\ztitlerefsetup{title=6} \zlabel{rule:metadata} \label{rule:metadata} \zrefused{rule:metadata}

Labels and build arguments can be very helpful to both provide metadata
and allow for customization of a build. This structured metadata is
machine-readable may help tools more than humans
(see~\hyperref[{rule:formatting}]{Rule~\ztitleref{rule:formatting}} for
user documentation within a \texttt{Dockerfile}). In addition to the
metadata creation by users, Docker captures useful information in the
image metadata automatically, such as the version of Docker used for
building the image, which you can access with the
\href{https://docs.docker.com/engine/reference/commandline/inspect/}{\texttt{docker\ inspect}}
command.

\hypertarget{labels}{%
\subsection{Labels}\label{labels}}

Labels serve as structured metadata that can be exposed by APIs, e.g.,
https://microbadger.com/labels, along with tools to inspect the
container binaries, e.g., \texttt{docker\ inspect}. For example,
software versions, maintainer contact information, along with vendor
specific metadata are commonly seen. The OCI Image Format Specification
provides some common label keys (see the ``Annotations'' section in
{[}45{]}) to help standardize field names across container tools, as
shown below. These labels match the \texttt{org.label-schema.*}
specification, which has been deprecated in favour or the new namespace
but are still found a lot in existing containers.

\begin{verbatim}
LABEL org.opencontainers.image.created='2019-12-10' \
  org.opencontainers.image.authors='author@example.org' \
  org.opencontainers.image.url='https://github.com/nuest/ten-simple-rules' \
  org.opencontainers.image.documentation='https://github.com/...' \
  org.opencontainers.image.version='0.0.1'

LABEL org.opencontainers.image.vendor='nuest' \
  org.opencontainers.image.title='Demo title' \
  org.opencontainers.image.description='Demo description'
\end{verbatim}

You can add multiple fields within the same instruction. Important
metadata attributes to include as labels would include any of the
following, ideally with globally unique identifiers:

\begin{itemize}
\tightlist
\item
  Author and contact (e.g., email, project website, or ORCID; you will
  often see the deprecated \texttt{MAINTAINER} instruction - use a label
  instead)
\item
  Research organizations (identified with https://ror.org/)
\item
  Funding agency/grant number
\item
  A repository link where the \texttt{Dockerfile} is published, e.g., a
  GitHub project or a repository record with a DOI, e.g., Zenodo, where
  you can pre-register a DOI and add it to your \texttt{Dockerfile}
  before publishing the record
\item
  License
\end{itemize}

Proper software citation is still a work in progress {[}46{]} and you
should follow current recommendations of projects such as CodeMeta
(\url{https://codemeta.github.io/}) for describing software and the
Citation File Format (\url{https://citation-file-format.github.io/}) to
enable citations of software.

\hypertarget{build-arguments}{%
\subsection{Build arguments}\label{build-arguments}}

Build arguments can provide more dynamic metadata and also allow for
customization of a build. As an example, the following build argument
would default to \texttt{1.0.0} but allow you to change it with
\texttt{-\/-build-arg\ MYVERSION=2.0.0} when building the image:

\begin{verbatim}
ARG MYVERSION=1.0.0
\end{verbatim}

Along with specifying versions, e.g., a git commit hash, or adding a
date and timestamp, build arguments can be useful to provide the context
of the build, e.g., building user, production versus development
environment, and automated or not. Examples of build arguments that are
useful to include to describe a container are:

\hypertarget{enable-interactive-usage-and-one-click-execution}{%
\section*{7. Enable interactive usage and one-click
execution}\label{enable-interactive-usage-and-one-click-execution}}
\addcontentsline{toc}{section}{7. Enable interactive usage and one-click
execution}

\ztitlerefsetup{title=7} \zlabel{rule:interactive} \label{rule:interactive} \zrefused{rule:interactive}

Containers are very well suited for day-to-day development tasks (see
also \hyperref[{rule:usage}]{Rule~\ztitleref{rule:usage}}), because they
support common interactive environments for data science and software
development. But they are also useful for a ``headless'' execution of a
full workflows, e.g.~as demonstrated in {[}47{]}. A workflow that does
not at all support headless execution may even be seen as not
reproducible. These two usages can be configured by the
\texttt{Dockerfile}'s author and exposed to the user based on the
\texttt{Dockerfile}'s \texttt{CMD} and \texttt{ENTRYPOINT} instructions
(see~\hyperref[{rule:templates}]{Rule~\ztitleref{rule:templates}} for
having multi-container environments). An images main purpose is
reflected by the default process and configuration, though the
\texttt{CMD} and \texttt{ENTRYPOINT} can also be changed at runtime. It
is considered good practice to have a combination of default command and
entrypoint that meets reasonable user expectations. For example, a
container known to be a workflow should execute the workflow, or provide
instructions for how to do so. Since you are likely the primary user of
the \texttt{Dockerfile} and image, so you should choose a selection or
combination that works for you, only then accomodate other user's needs.
A possible weakness with using containers is the limitation on only
providing one default command and entrypoint. However tools, e.g., The
Scientific Filesystem {[}48{]}, have been developed to expose multiple
entrypoints, environments, help messages, labels, and even install
sequences. With plain Docker, you can override the defaults as part of
the \texttt{docker\ run} command or in an extra \texttt{Dockerfile}
using the primary image as a base. In any case you should document
different variants, if you choose to provide them, in a
\texttt{Makefile} {[}49{]}. To support both one click execution and
interactive interfaces and even allow for custom configuration, it's
helpful to expose settings via a configuration file which can be bound
from the host, via environment variables {[}50{]}, or special
Docker-based wrappers such as Kliko {[}51{]}.

\emph{Interactive graphical interfaces}, such as
\href{https://rstudio.com/products/rstudio/}{RStudio},
\href{https://jupyter.org/}{Jupyter}, or
\href{https://code.visualstudio.com/}{Visual Studio Code}, can run in a
container to be used across platforms via a regular web browser. The
HTML-based user interface is exposed over HTTP. Use the \texttt{EXPOSE}
instruction to document the ports of interest for both humans and tools,
because they need to be bound to the host to be accessible to the user
using the \texttt{docker\ run} option
\texttt{-p}/\texttt{-\/-publish\ \textless{}host\ port\textgreater{}:\textless{}container\ port\textgreater{}}.
The container should also print to the screen the used ports along with
any login credentials needed. For example, as done in the last few lines
of the output of running a Jupyter Notebook server locally (lines
abbreviated).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ run -p 8888:8888 jupyter/datascience-notebook:7a0c7325e470}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[...]
[I 15:44:31.323 NotebookApp] The Jupyter Notebook is running at:
[I 15:44:31.323 NotebookApp] http://9027563c6465:8888/?token=6a92d [..]
[I 15:44:31.323 NotebookApp]  or http://127.0.0.1:8888/?token=6a92 [..]
[I 15:44:31.323 NotebookApp] Use Control-C to stop this server and [..]
\end{verbatim}

A person who is unfamiliar with Docker but wants to use your image may
rely on graphical tools like Kitematic {[}52{]} or
\href{https://containds.com/}{ContainDS} for assisstance in managing
containers on their machine without using the Docker CLI.

\emph{Interactive usage of a command-line interfaces} is quite
straightforward to access from containers, if users are familiar with
this style of user interface. Running the container will provide a shell
where a tool can be used and help or error messages can assist the user.
For example, complex workflows in any programming language can, with
suitable pre-configuration, be triggered by running a specific script
file. If your workflow can be executed via a CLI you may use that to
validate correct functionality of an image in automated builds,
e.g.~using a small toy example and checking the output, by checking
successful responses from HTTP endpoints provided by the container,
e.g.~via an HTTP response code of \texttt{200}, or by using a controller
such as Selenium {[}53{]}.

The followig example runs a simple R command counting the lines in this
articles source file. The file path is passed as an environment
variable.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ run \textbackslash{}}
\NormalTok{  --env CONFIG_PARAM=}\StringTok{"/data/ten-simple-rules-dockerfiles.Rmd"}\NormalTok{ \textbackslash{}}
\NormalTok{  --volume }\VariableTok{$(}\BuiltInTok{pwd}\VariableTok{)}\NormalTok{:/data \textbackslash{}}
\NormalTok{  jupyter/datascience-notebook:7a0c7325e470 \textbackslash{}}
\NormalTok{  R --quiet -e }\StringTok{"}
\StringTok{l = length(readLines(Sys.getenv('CONFIG_PARAM')));}
\StringTok{print(paste('Number of lines: ', l))}
\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> l = length(readLines(Sys.getenv('CONFIG_PARAM')));
> print(paste('Number of lines: ', l))
[1] "Number of lines:  568"
\end{verbatim}

If there is only a regular desktop application, the hosts window manager
can be connected to the container. This has notable security
implications, which are reduced by using the ``X11 forwarding'' natively
supported by Singularity {[}54{]}, which can execute Docker containers,
or by leveraging supporting tools such as \texttt{x11docker} {[}55{]}.
Bidge containers {[}56{]} and exposing a regular desktop via the browser
(e.g., for Jupyter Hub {[}57{]}) are further alternatives. This variety
of approaches render seemingly more convenient uncontainerised
environments, i.e.~just using the local machine, unneccessary in favour
of reproducibility and portability.

\hypertarget{establish-templates-for-new-projects}{%
\section*{8. Establish templates for new
projects}\label{establish-templates-for-new-projects}}
\addcontentsline{toc}{section}{8. Establish templates for new projects}

\ztitlerefsetup{title=8} \zlabel{rule:templates} \label{rule:templates} \zrefused{rule:templates}

It is likely going to be the case that over time you will develop
workflows that are similar in nature to one another. In this case, you
should consider adopting a standard workflow that will give you a clean
slate for a new project. If you decide to build your own standard,
collaborate with your community during development of the standard to
ensure it will be useful to others. Part of your project template should
be a protocol for publishing the container image to a suitable container
registry, and taking into consideration of how the code can be given a
DOI or proper publication (e.g., Zenodo, Journal of Open Source
Software).

As an example, cookie cutter templates {[}58{]}, project starter kits
{[}REF{]}, or community templates (e.g., {[}59{]}) can provide files,
directory organization, and build instructions that include basic steps
for getting started. A good project template should get you started with
a template for documentation, setting up testing via continuous
integration (CI), building a container, and even choosing a license. In
the case of using a common \texttt{Dockerfile} or base, Docker's build
caching will take shared lines into account and speed up build time even
between projects. When developing or working on projects with containers
you can switch between isolated project environments by stopping the
container and restarting it when you are ready to work again, even on
another machine or in a cloud environment. You can even run projects in
parallel without interference. At most, if a port is shared by two
projects to expose a user interface in the browser, you would need to
configure non-conflicting ports.

In the case of more complex applications that require web servers,
databases, and workers or messaging, the entire infrastructure can be
brought up or down with a combination of templates and orchestration
tools like \texttt{docker-compose} {[}60{]}. \texttt{docker-compose}
also allows definition of services using multiple containers via it's
own \texttt{docker-compose.yml} file. This file can help to template
options including mounted volumes, permissions, environment variables,
and exposed ports. A \texttt{docker-compose} configuration also helps to
keep one \emph{purpose} per container, which often means one process
running in the container, and to combine existing stable building blocks
instead of bespoke massive containers for specific purposes.

\hypertarget{publish-one-dockerfile-per-project-in-a-code-repository-with-version-control}{%
\section*{9. Publish one Dockerfile per project in a code repository
with version
control}\label{publish-one-dockerfile-per-project-in-a-code-repository-with-version-control}}
\addcontentsline{toc}{section}{9. Publish one Dockerfile per project in
a code repository with version control}

\ztitlerefsetup{title=9} \zlabel{rule:publish} \label{rule:publish} \zrefused{rule:publish}

Because a \texttt{Dockerfile} is a plain text-based format, it works
well with VCS. Including a \texttt{Dockerfile} alongside your code and
data is an effective way to consistently build your software, to show
visitors to the repository how it is built and used, to solicit feedback
and collaborate with your peers, and to increase the impact and
sustainability of your work (cf.~{[}61{]}). Online collaboration
platforms (e.g., GitHub, GitLab) also make it easy to use CI services to
test building your image in an independent build environment. Continuous
integration increases stability and trust, and gives the ability to
publish images automatically. If your \texttt{Dockerfile} includes an
interactive user interface, you can also adapt it so that it is
ready-to-use as a Binder instance {[}17{]}, providing an online work
environment to any user with a simple click of a link. Furthermore, the
commit messages in your version controlled repository preserve a record
of all changes to the \texttt{Dockerfile}.

While there are exceptions to the rule (cf.~{[}62{]}), it's generally a
simple and clear approach to provide one \texttt{Dockerfile} per
project. Alternatively, you could build an \emph{image stack} to serve
different use cases with a common base image, but this reduces
understandability and scatters information across multiple places. We
recommend to avoid this at the price of a longer \texttt{Dockerfile} and
mitigate with consistent form
(see~\hyperref[{rule:formatting}]{Rule~\ztitleref{rule:formatting}}). If
you find that you need to use more than one container to run your
workflow, use \texttt{docker-compose} and consider if it's possible to
use build arguments to flip between states (e.g., development
vs.~production,
see~\hyperref[{rule:interactive}]{Rule~\ztitleref{rule:interactive}}) or
to separate tools into different repositories. Yet most importantly, you
should publish \emph{all} files \texttt{COPY}ied into the container,
e.g., test data or files for software installation from source
(see~\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}), in the same
repository.

\hypertarget{use-the-container-daily-rebuild-the-image-weekly-clean-up-and-preserve-if-need-be}{%
\section*{10. Use the container daily, rebuild the image weekly, clean
up and preserve if need
be}\label{use-the-container-daily-rebuild-the-image-weekly-clean-up-and-preserve-if-need-be}}
\addcontentsline{toc}{section}{10. Use the container daily, rebuild the
image weekly, clean up and preserve if need be}

\ztitlerefsetup{title=10} \zlabel{rule:usage} \label{rule:usage} \zrefused{rule:usage}

Using containers for research workflows does not only require technical
understanding, but also an awareness of risks that can be managed
efficiently by following a number of good \emph{habits}, which we
outline below. While there is no firm rule, if you use a container
daily, is good practice to rebuild that container every once or two
weeks. At the time of publication of research results it is good
practice to save a copy of the image in a public data repository so that
readers of the publication can access the resources that produced the
published results.

First, use your container every time you work on a project and not just
as a final step during publication. If the container is the only
platform you use, the confidence in proper documentation of the
computing environment can be very high {[}63{]}. You should prioritize
this usage over others, e.g., non-interactive execution of a full
workflow, because it gives you personally the highest value and does not
limit your use or others' use of your data and code at all (see
\hyperref[{rule:interactive}]{Rule~\ztitleref{rule:interactive}}).

Second, for reproducibility, we can treat containers as transient and
disposable, and even intentionally rebuild an image at regular
intervals. Ideally, containers that we built years ago should rebuild
seamlessly, but this is not necessarily the case, especially with
rapidly changing technology relevant to machine learning and data
science. It can almost be guaranteed future changes will interfere with
the image build process or even the functionality of the software within
the image. The longer that you wait to rebuild the image, the more
change happens, the higher the risk that your build will not work, and
the harder it becomes to identify and fix the problem. If you are using
an interactive container and find that you need to manually install a
package or change a parameter, it is best practice to add this
dependency to the container and rebuild it right away, but one tends to
take shortcuts. Therefore, a habitual deletion of a container and
cache-less rebuild of the image (a) increases security due to updating
underlying software, (b) helps to reveals issues requiring manual
interference, i.e., changes to code or configuration not documented in
the \texttt{Dockerfile} but perhaps should be, and (c) allows you to
more incrementally debug issues. This habit can be supported by using
continuous deployment or CI strategies. If a VCS repository with a
\texttt{Dockerfile} is linked to an automated build, then a CI pipeline
can build images and execute a validating run the new image, e.g., using
contained demo data
(see~\hyperref[{rule:mount}]{Rule~\ztitleref{rule:mount}}).

In the case of needing setup or configuration for the first two habits,
it is good practice to provide a \texttt{Makefile} alongside your
container, which can capture the specific commands. The effective use of
a \texttt{Makefile} can help avoiding undocumented steps or manual,
extra commands to be run on the local machine. A fully scripted
configuration makes it easier for both you and future users, and can
increase trust in your workflow.

Third, from time to time you can reduce the system resources occupied by
Docker images and their layers or unused containers, volumes and
networks by running \texttt{docker\ system\ prune\ -\/-all}. After a
prune is performed, it follows naturally to rebuild a container for
local usage, or to pull it again from a newly built registry image. This
habit can be automated with a cron job {[}64{]}.

Fourth, you can export the image to file and deposit it in a public data
repository, where it not only becomes citable but also provides a
snapshot of the \emph{actual} environment you used at a specific point
in time. You should include instructions how to import and run the
workflow based on the image archive and add your own image tags for
clarity. Depositing the image next to other project files, i.e., data,
code, and the used \texttt{Dockerfile}, in a public repository makes
them likely to be preserved, but is is highly unlikely that over time
you will be able to recreate it precisely from the accompanying
\texttt{Dockerfile}. Publishing the image and the contained metadata
therein (e.g., the Docker version used) may even allow future science
historians to emulate the Docker runtime environment. Applying proper
preservation strategies (cf.~{[}61{]}) can be highly complex, but simply
running an image ``as-is'', i.e.~with the default command and entrypoint
(see \hyperref[{rule:interactive}]{Rule~\ztitleref{rule:interactive}}),
and observing the output is quite likely to work for many years into the
future. If the image does not work anymore, a user can still extract the
image contents and explore the files of each layer manually, or if an
import still works, with exploration tools like dive {[}37{]}. However,
if you want to ensure usability and extendability, then you could run
import, run, and export an image regularly to make sure the export
format still works with the then current version of Docker.

The exported image and a version controlled \texttt{Dockerfile} together
allow you to freely experiment and continue development of your workflow
and keeping the image up to date, e.g., updating versions of pinned
dependencies (see
\hyperref[{rule:pinning}]{Rule~\ztitleref{rule:pinning}}) and regular
image building (see above).

Finally, for a sanity check and to foster even higher trust in the
stability and documentation of your project, you can ask a colleague or
community member to be your code copilot (see
\url{https://twitter.com/Code_Copilot}) to interact with your workflow
container on a machine of their own. You can do this shortly before
submitting your reproducible workflow for peer-review, so you are well
positioned for the future of scholarly communication and open science
where these may be standard practices required for publication
{[}22,65--67{]}.

\hypertarget{example-dockerfiles}{%
\section{Example Dockerfiles}\label{example-dockerfiles}}

To demonstrate the ten rules, we maintain a GitHub repository with
example \texttt{Dockerfile}s, some of which we took from public
repositories and updated to adhere to the rules (see
\texttt{Dockerfile.original}):
\url{https://github.com/nuest/ten-simple-rules-dockerfiles/}

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

Reproducibility in research is an endeavor of incremental improvement
and best efforts, not about achieving the perfect solution, which may be
not achievable for many researchers with limited resources, and the
definition of which may change over time. Containerisation can support
researchers. Containers can be optimized for very different use cases,
such as supporting many hosts, being small in size for quick deployments
in cloud infrastructures, but in this article we have provided guidance
for using \texttt{Dockerfile}s to create containers for use and
communication in computational research. Our goal is to help the
researcher to work towards creating a ``time capsule'' {[}68{]} in form
of a research compendium. Given some expertise and the right tools, such
a compendium can come as close as needed to the original workflow with
reasonably little effort for better science. Even if such a capsule
decays over time, the effort to create and document it provides
incredibly useful and valuable transparency for the project. We
encourage researchers to value these steps taken by their peers to use
\texttt{Dockerfile}s to create ``time capsules'', and promote change in
the way scholars communicate towards an open and friendly
``preproducibility'' {[}69{]}. So please, make a best effort with your
current knowledge, and strive to write readable \texttt{Dockerfile}s for
functional containers, that are realistic about what might break and
what is unlikely to break. In a similar vein, we accept that you should
freely break these rules if another way makes more sense \emph{for your
use case}. Most importantly, share and exchange your \texttt{Dockerfile}
freely and collaborate in your community to spread the knowledge about
containers as a tool for research and scholarly collaboration and
communication. Together we can develop common practices and shared
materials for better transparency, higher efficiency, and faster
innovation.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

DN is supported by the project Opening Reproducible Research II
(\href{https://o2r.info/}{https://o2r.info/};
\href{https://www.uni-muenster.de/forschungaz/project/12343}{https://www.uni-muenster.de/forschungaz/project/12343})
funded by the German Research Foundation (DFG) under project number PE
1632/17-1.

\hypertarget{contributions}{%
\section*{Contributions}\label{contributions}}
\addcontentsline{toc}{section}{Contributions}

DN conceived the idea, outlined the first rules, and contributed to all
rules. VS wrote the first draft and contributed to all rules. BM revised
the text and contributed to all rules. SE contributed to the overall
structure and selected rules. THe contributed to the rule structure and
particularly Rule~1. THi gave extensive feedback on early drafts and
contributed to the discussion. This articles was written collaboratively
on GitHub, where all
\href{https://github.com/nuest/ten-simple-rules-dockerfiles/graphs/contributors}{contributions
in form of text or discussions comments} are documented:
\url{https://github.com/nuest/ten-simple-rules-dockerfiles/}.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-wikipedia_contributors_version_2019}{}%
1. Wikipedia contributors. Version control {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Version_control\&oldid=926593231}

\leavevmode\hypertarget{ref-marwick_how_2015}{}%
2. Marwick B. How computers broke science -- and what we can do to fix
it {[}Internet{]}. The Conversation. 2015. Available:
\url{https://theconversation.com/how-computers-broke-science-and-what-we-can-do-to-fix-it-49938}

\leavevmode\hypertarget{ref-donoho_invitation_2010}{}%
3. Donoho DL. An invitation to reproducible computational research.
Biostatistics. 2010;11: 385--388.
doi:\href{https://doi.org/10.1093/biostatistics/kxq028}{10.1093/biostatistics/kxq028}

\leavevmode\hypertarget{ref-wilson_best_2014}{}%
4. Wilson G, Aruliah DA, Brown CT, Hong NPC, Davis M, Guy RT, et al.
Best Practices for Scientific Computing. PLOS Biology. 2014;12:
e1001745.
doi:\href{https://doi.org/10.1371/journal.pbio.1001745}{10.1371/journal.pbio.1001745}

\leavevmode\hypertarget{ref-wilson_good_2017}{}%
5. Wilson G, Bryan J, Cranston K, Kitzes J, Nederbragt L, Teal TK. Good
enough practices in scientific computing. PLOS Computational Biology.
2017;13: e1005510.
doi:\href{https://doi.org/10.1371/journal.pcbi.1005510}{10.1371/journal.pcbi.1005510}

\leavevmode\hypertarget{ref-rule_ten_2019}{}%
6. Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et
al. Ten simple rules for writing and sharing computational analyses in
Jupyter Notebooks. PLOS Computational Biology. 2019;15: e1007007.
doi:\href{https://doi.org/10.1371/journal.pcbi.1007007}{10.1371/journal.pcbi.1007007}

\leavevmode\hypertarget{ref-sandve_ten_2013}{}%
7. Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for
Reproducible Computational Research. PLoS Comput Biol. 2013;9: e1003285.
doi:\href{https://doi.org/10.1371/journal.pcbi.1003285}{10.1371/journal.pcbi.1003285}

\leavevmode\hypertarget{ref-nust_author_2017}{}%
8. Nüst D. Author Carpentry : Docker for reproducible research
{[}Internet{]}. Author Carpentry : Docker for reproducible research.
2017. Available:
\url{https://nuest.github.io/docker-reproducible-research/}

\leavevmode\hypertarget{ref-chapman_reproducible_2018}{}%
9. Chapman P. Reproducible data science environments with Docker Phil
Chapman's Blog {[}Internet{]}. 2018. Available:
\url{https://chapmandu2.github.io/post/2018/05/26/reproducible-data-science-environments-with-docker/}

\leavevmode\hypertarget{ref-ropensci_labs_r_2015}{}%
10. rOpenSci Labs. R Docker tutorial {[}Internet{]}. 2015. Available:
\url{https://ropenscilabs.github.io/r-docker-tutorial/}

\leavevmode\hypertarget{ref-udemy_docker_2019}{}%
11. Udemy, Zhbanko V. Docker Containers for Data Science and
Reproducible Research {[}Internet{]}. Udemy. 2019. Available:
\url{https://www.udemy.com/course/docker-containers-data-science-reproducible-research/}

\leavevmode\hypertarget{ref-psomopoulos_lesson_2017}{}%
12. Psomopoulos FE. Lesson "Docker and Reproducibility" in Workshop
"Reproducible analysis and Research Transparency" {[}Internet{]}.
Reproducible analysis and Research Transparency. 2017. Available:
\url{https://reproducible-analysis-workshop.readthedocs.io/en/latest/8.Intro-Docker.html}

\leavevmode\hypertarget{ref-docker_inc_best_2020}{}%
13. Docker Inc. Best practices for writing Dockerfiles {[}Internet{]}.
Docker Documentation. 2020. Available:
\url{https://docs.docker.com/develop/develop-images/dockerfile_best-practices/}

\leavevmode\hypertarget{ref-brinckman_computing_2018}{}%
14. Brinckman A, Chard K, Gaffney N, Hategan M, Jones MB, Kowalik K, et
al. Computing environments for reproducibility: Capturing the ``Whole
Tale''. Future Generation Computer Systems. 2018;
doi:\href{https://doi.org/10.1016/j.future.2017.12.029}{10.1016/j.future.2017.12.029}

\leavevmode\hypertarget{ref-code_ocean_2019}{}%
15. Code Ocean {[}Internet{]}. 2019. Available:
\url{https://codeocean.com/}

\leavevmode\hypertarget{ref-simko_reana_2019}{}%
16. Šimko T, Heinrich L, Hirvonsalo H, Kousidis D, Rodríguez D. REANA: A
System for Reusable Research Data Analyses. EPJ Web of Conferences.
2019;214: 06034.
doi:\href{https://doi.org/10.1051/epjconf/201921406034}{10.1051/epjconf/201921406034}

\leavevmode\hypertarget{ref-jupyter_binder_2018}{}%
17. Jupyter P, Bussonnier M, Forde J, Freeman J, Granger B, Head T, et
al. Binder 2.0 - Reproducible, interactive, sharable environments for
science at scale. Proceedings of the 17th Python in Science Conference.
2018; 113--120.
doi:\href{https://doi.org/10.25080/Majora-4af1f417-011}{10.25080/Majora-4af1f417-011}

\leavevmode\hypertarget{ref-nust_opening_2017}{}%
18. Nüst D, Konkol M, Pebesma E, Kray C, Schutzeichel M, Przibytzin H,
et al. Opening the Publication Process with Executable Research
Compendia. D-Lib Magazine. 2017;23.
doi:\href{https://doi.org/10.1045/january2017-nuest}{10.1045/january2017-nuest}

\leavevmode\hypertarget{ref-cohen_four_2020}{}%
19. Cohen J, Katz DS, Barker M, Chue Hong NP, Haines R, Jay C. The Four
Pillars of Research Software Engineering. IEEE Software. 2020;
doi:\href{https://doi.org/10.1109/MS.2020.2973362}{10.1109/MS.2020.2973362}

\leavevmode\hypertarget{ref-wikipedia_contributors_docker_2019}{}%
20. Wikipedia contributors. Docker (software) {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Docker_(software)\&oldid=928441083}

\leavevmode\hypertarget{ref-boettiger_introduction_2017}{}%
21. Boettiger C, Eddelbuettel D. An Introduction to Rocker: Docker
Containers for R. The R Journal. 2017;9: 527--536.
doi:\href{https://doi.org/10.32614/RJ-2017-065}{10.32614/RJ-2017-065}

\leavevmode\hypertarget{ref-chen_open_2019}{}%
22. Chen X, Dallmeier-Tiessen S, Dasler R, Feger S, Fokianos P, Gonzalez
JB, et al. Open is not enough. Nature Physics. 2019;15: 113.
doi:\href{https://doi.org/10.1038/s41567-018-0342-2}{10.1038/s41567-018-0342-2}

\leavevmode\hypertarget{ref-docker_inc_dockerfile_2019}{}%
23. Docker Inc. Dockerfile reference {[}Internet{]}. Docker
Documentation. 2019. Available:
\url{https://docs.docker.com/engine/reference/builder/}

\leavevmode\hypertarget{ref-kurtzer_singularity_2017}{}%
24. Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers
for mobility of compute. PLOS ONE. 2017;12: e0177459.
doi:\href{https://doi.org/10.1371/journal.pone.0177459}{10.1371/journal.pone.0177459}

\leavevmode\hypertarget{ref-boettiger_introduction_2015}{}%
25. Boettiger C. An Introduction to Docker for Reproducible Research.
SIGOPS Oper Syst Rev. 2015;49: 71--79.
doi:\href{https://doi.org/10.1145/2723872.2723882}{10.1145/2723872.2723882}

\leavevmode\hypertarget{ref-marwick_madjebebe_2015}{}%
26. Ben Marwick. 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-docker_inc_official_2019}{}%
27. Docker Inc. Official Images on Docker Hub {[}Internet{]}. Docker
Documentation. 2019. Available:
\url{https://docs.docker.com/docker-hub/official_images/}

\leavevmode\hypertarget{ref-nust_containerit_2019}{}%
28. Nüst D, Hinz M. Containerit: Generating Dockerfiles for reproducible
research with R. Journal of Open Source Software. 2019;4: 1603.
doi:\href{https://doi.org/10.21105/joss.01603}{10.21105/joss.01603}

\leavevmode\hypertarget{ref-stencila_dockta_2019}{}%
29. Stencila. Stencila/dockta {[}Internet{]}. Stencila; 2019. Available:
\url{https://github.com/stencila/dockta}

\leavevmode\hypertarget{ref-docker_inc_official_2020}{}%
30. Docker Inc. Official Images on Docker Hub {[}Internet{]}. Docker
Documentation. 2020. Available:
\url{https://docs.docker.com/docker-hub/official_images/}

\leavevmode\hypertarget{ref-husain_repo2docker-action_2019}{}%
31. Husain H, Silkaitis R. Machine-learning-apps/repo2docker-action
{[}Internet{]}. ML Apps; 2019. Available:
\url{https://github.com/machine-learning-apps/repo2docker-action}

\leavevmode\hypertarget{ref-scottyhq_repo2docker-githubci_2019}{}%
32. Henderson S. Scottyhq/repo2docker-githubci {[}Internet{]}. 2019.
Available: \url{https://github.com/scottyhq/repo2docker-githubci}

\leavevmode\hypertarget{ref-preston-werner_semantic_2013}{}%
33. Preston-Werner T. Semantic Versioning 2.0.0 {[}Internet{]}. Semantic
Versioning. 2013. Available: \url{https://semver.org/}

\leavevmode\hypertarget{ref-docker_multi-stage_2020}{}%
34. Docker Inc. Use multi-stage builds {[}Internet{]}. Docker
Documentation. 2020. Available:
\url{https://docs.docker.com/develop/develop-images/multistage-build/}

\leavevmode\hypertarget{ref-halchenko_open_2012}{}%
35. Halchenko YO, Hanke M. Open is Not Enough. Let's Take the Next Step:
An Integrated, Community-Driven Computing Platform for Neuroscience.
Frontiers in Neuroinformatics. 2012;6.
doi:\href{https://doi.org/10.3389/fninf.2012.00022}{10.3389/fninf.2012.00022}

\leavevmode\hypertarget{ref-wikipedia_contributors_lint_2019}{}%
36. Wikipedia contributors. Lint (software) {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Lint_(software)\&oldid=907589761}

\leavevmode\hypertarget{ref-goodman_dive_2019}{}%
37. Goodman A. Wagoodman/dive {[}Internet{]}. 2019. Available:
\url{https://github.com/wagoodman/dive}

\leavevmode\hypertarget{ref-the_python_software_foundation_requirements_2019}{}%
38. The Python Software Foundation. Requirements Files --- pip User
Guide {[}Internet{]}. 2019. Available:
\url{https://pip.pypa.io/en/stable/user_guide/\#requirements-files}

\leavevmode\hypertarget{ref-continuum_analytics_managing_2017}{}%
39. Continuum Analytics. Managing environments --- conda documentation
{[}Internet{]}. 2017. Available:
\url{https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html}

\leavevmode\hypertarget{ref-r_core_team_description_1999}{}%
40. R Core Team. The DESCRIPTION file in "writing r extensions"
{[}Internet{]}. 1999. Available:
\url{https://cran.r-project.org/doc/manuals/r-release/R-exts.html\#The-DESCRIPTION-file}

\leavevmode\hypertarget{ref-eddelbuettel_littler_2019}{}%
41. Eddelbuettel D, Horner J. Littler: R at the command-line via 'r'
{[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=littler}

\leavevmode\hypertarget{ref-npm_creating_2019}{}%
42. npm. Creating a package.json file npm Documentation {[}Internet{]}.
2019. Available:
\url{https://docs.npmjs.com/creating-a-package-json-file}

\leavevmode\hypertarget{ref-julia_tomls_2019}{}%
43. The Julia Language Contributors. 10. Project.Toml and Manifest.Toml
· Pkg.Jl {[}Internet{]}. 2019. Available:
\url{https://julialang.github.io/Pkg.jl/v1/toml-files/}

\leavevmode\hypertarget{ref-docker_use_2019}{}%
44. Docker Inc. Use bind mounts {[}Internet{]}. Docker Documentation.
2019. Available: \url{https://docs.docker.com/storage/bind-mounts/}

\leavevmode\hypertarget{ref-opencontainers_image-spec_2017}{}%
45. Opencontainers. Opencontainers/image-spec v1.0.1 - Annotations
{[}Internet{]}. GitHub. 2017. Available:
\url{https://github.com/opencontainers/image-spec/blob/v1.0.1/annotations.md}

\leavevmode\hypertarget{ref-katz_software_2018}{}%
46. Katz DS, Chue Hong NP. Software Citation in Theory and Practice. In:
Davenport JH, Kauers M, Labahn G, Urban J, editors. Mathematical
Software -- ICMS 2018. Springer International Publishing; 2018. pp.
289--296.
doi:\href{https://doi.org/10.1007/978-3-319-96418-8_34}{10.1007/978-3-319-96418-8\_34}

\leavevmode\hypertarget{ref-verstegen_pluc_mozambique_2019}{}%
47. Verstegen JA. JudithVerstegen/PLUC\_Mozambique: First release of
PLUC for Mozambique {[}Internet{]}. Zenodo; 2019.
doi:\href{https://doi.org/10.5281/zenodo.3519987}{10.5281/zenodo.3519987}

\leavevmode\hypertarget{ref-sochat_scientific_2018}{}%
48. Sochat V. The Scientific Filesystem. GigaScience. 2018;7.
doi:\href{https://doi.org/10.1093/gigascience/giy023}{10.1093/gigascience/giy023}

\leavevmode\hypertarget{ref-wikipedia_contributors_make_2019}{}%
49. Wikipedia contributors. Make (software) {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Make_(software)\&oldid=929976465}

\leavevmode\hypertarget{ref-knoth_reproducibility_2017}{}%
50. Knoth C, Nüst D. Reproducibility and Practical Adoption of GEOBIA
with Open-Source Software in Docker Containers. Remote Sensing. 2017;9:
290. doi:\href{https://doi.org/10.3390/rs9030290}{10.3390/rs9030290}

\leavevmode\hypertarget{ref-molenaar_klikoscientific_2018}{}%
51. Molenaar G, Makhathini S, Girard JN, Smirnov O. Kliko---The
scientific compute container format. Astronomy and Computing. 2018;25:
1--9.
doi:\href{https://doi.org/10.1016/j.ascom.2018.08.003}{10.1016/j.ascom.2018.08.003}

\leavevmode\hypertarget{ref-docker_kitematic_2019}{}%
52. Docker Inc. Docker/kitematic {[}Internet{]}. Docker; 2019.
Available: \url{https://github.com/docker/kitematic}

\leavevmode\hypertarget{ref-selenium_2019}{}%
53. Selenium contributors. SeleniumHQ/selenium {[}Internet{]}. Selenium;
2019. Available: \url{https://github.com/SeleniumHQ/selenium}

\leavevmode\hypertarget{ref-singularity_frequently_2019}{}%
54. Singularity. Frequently Asked Questions Singularity {[}Internet{]}.
2019. Available:
\url{http://singularity.lbl.gov/archive/docs/v2-2/faq\#can-i-run-x11-apps-through-singularity}

\leavevmode\hypertarget{ref-viereck_x11docker_2019}{}%
55. Viereck M. X11docker: Run GUI applications in Docker containers.
Journal of Open Source Software. 2019;4: 1349.
doi:\href{https://doi.org/10.21105/joss.01349}{10.21105/joss.01349}

\leavevmode\hypertarget{ref-yaremenko_docker-x11-bridge_2019}{}%
56. Yaremenko E. JAremko/docker-x11-bridge {[}Internet{]}. 2019.
Available: \url{https://github.com/JAremko/docker-x11-bridge}

\leavevmode\hypertarget{ref-yuvipanda_jupyter-desktop-server_2019}{}%
57. Panda Y. Yuvipanda/jupyter-desktop-server {[}Internet{]}. 2019.
Available: \url{https://github.com/yuvipanda/jupyter-desktop-server}

\leavevmode\hypertarget{ref-cookiecutter_contributors_cookiecutter_2019}{}%
58. \{Cookiecutter contributors\}. Cookiecutter/cookiecutter
{[}Internet{]}. cookiecutter; 2019. Available:
\url{https://github.com/cookiecutter/cookiecutter}

\leavevmode\hypertarget{ref-marwick_rrtools_2019}{}%
59. Marwick B. Benmarwick/rrtools {[}Internet{]}. 2019. Available:
\url{https://github.com/benmarwick/rrtools}

\leavevmode\hypertarget{ref-docker-compose_2019}{}%
60. Docker Inc. Overview of Docker Compose {[}Internet{]}. Docker
Documentation. 2019. Available: \url{https://docs.docker.com/compose/}

\leavevmode\hypertarget{ref-emsley_framework_2018}{}%
61. Emsley I, De Roure D. A Framework for the Preservation of a Docker
Container International Journal of Digital Curation. International
Journal of Digital Curation. 2018;12.
doi:\href{https://doi.org/10.2218/ijdc.v12i2.509}{10.2218/ijdc.v12i2.509}

\leavevmode\hypertarget{ref-kim_bio-docklets_2017}{}%
62. Kim B, Ali TA, Lijeron C, Afgan E, Krampis K. Bio-Docklets:
Virtualization Containers for Single-Step Execution of NGS Pipelines.
bioRxiv. 2017; 116962.
doi:\href{https://doi.org/10.1101/116962}{10.1101/116962}

\leavevmode\hypertarget{ref-marwick_readme_2015}{}%
63. Marwick B. README of 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-wikipedia_contributors_cron_2019}{}%
64. Wikipedia contributors. Cron {[}Internet{]}. Wikipedia. 2019.
Available:
\url{https://en.wikipedia.org/w/index.php?title=Cron\&oldid=929379536}

\leavevmode\hypertarget{ref-eglen_codecheck_2019}{}%
65. Eglen S, Nüst D. CODECHECK: An open-science initiative to facilitate
sharing of computer programs and results presented in scientific
publications. Septentrio Conference Series. 2019;
doi:\href{https://doi.org/10.7557/5.4910}{10.7557/5.4910}

\leavevmode\hypertarget{ref-schonbrodt_training_2019}{}%
66. Schönbrodt F. Training students for the Open Science future. Nature
Human Behaviour. 2019;3: 1031--1031.
doi:\href{https://doi.org/10.1038/s41562-019-0726-z}{10.1038/s41562-019-0726-z}

\leavevmode\hypertarget{ref-eglen_recent_2018}{}%
67. Eglen SJ, Mounce R, Gatto L, Currie AM, Nobis Y. Recent developments
in scholarly publishing to improve research practices in the life
sciences. Emerging Topics in Life Sciences. 2018;2: 775--778.
doi:\href{https://doi.org/10.1042/ETLS20180172}{10.1042/ETLS20180172}

\leavevmode\hypertarget{ref-blank_twitter_2019}{}%
68. Blank D, Twitter. Twitter thread on reproducibility time capsules on
Twitter {[}Internet{]}. Twitter. 2019. Available:
\url{https://twitter.com/dougblank/status/1135904909663068165}

\leavevmode\hypertarget{ref-stark_before_2018}{}%
69. Stark PB. Before reproducibility must come preproducibility
{[}Internet{]}. Nature. 2018.
doi:\href{https://doi.org/10.1038/d41586-018-05256-0}{10.1038/d41586-018-05256-0}

\nolinenumbers


\end{document}

