% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}




\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Writing Dockerfiles for Reproducible Research} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Daniel Nüst\textsuperscript{\getAff{Institute for Geoinformatics, University of M``unster, M''unster,
Germany}}\textsuperscript{*},
Vanessa Sochat\textsuperscript{\getAff{Stanford Research Comuting Center, Stanford University, Stanford, CA, US}},
Stephen Eglen\textsuperscript{\getAff{Department of Applied Mathematics and Theoretical Physics, University of
Cambridge, Cambridge, Cambridgeshire, GB}},
Tim Head\textsuperscript{\getAff{Wild Tree Tech, Zurich, CH}}\\
\bigskip
\bigskip
* Corresponding author: daniel.nuest@uni-muenster.de\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Containers are greatly improving computational science by packaging
software and data dependencies. In a scholarly context, transparency and
support of reproducibility are the largest drivers for using these
containers. It follows that choices that are made with respect to
building containers can make or break its reproducibility. The build for
the container image is often created based on the instructions in a
plain-text file. For example, one such container technology, Docker,
provides instructions using a \texttt{Dockerfile}. By following the
rules in this article researchers writing a \texttt{Dockerfile} can
effectively build and distribute containers.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.
\section*{Author summary}
TBD

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

With access to version control systems {[}REF GITHUB, GITLAB, etc.{]} it
has become increasingly easy to not only share algorithms, but also
instructions for building and testing. Within this content, it is
suggested to include instructions for building containers to package
software and data dependencies {[}1{]}. By way of providing these
instructions along with thorough documentation, it's much more likely to
be able to reproduce an analysis workflow. A container is a portabal,
encapsulated environment that is built from human and machine readable
instructions. For example, containers have been created to host
scientific notebooks {[}2{]}, and share reproducible workflows (cf.~Rule
10 of {[}3{]}).

While there are several tutorials for using containers for reproducible
research,\footnote{https://nuest.github.io/docker-reproducible-research/,
  https://chapmandu2.github.io/post/2018/05/26/reproducible-data-science-environments-with-docker/,
  https://reproducible-analysis-workshop.readthedocs.io/en/latest/8.Intro-Docker.html}
there is no extensive examination for how to write the actual
instructions to create the containers. Several platforms for
facilitating reproducible research are built on top of containers
{[}4--8{]}, but they hide any complexity from the researcher. While not
everybody needs to understand and write Dockerfiles, as \emph{``the
number of unique research environments approximates the number of
researchers''} {[}8{]}, the researchers who do need to craft their own
environment definition should follow good practices. Such practices are
not part of generic Docker tutorials and are not present in existing
Dockerfiles often used as templates. The differences and potential
obstacles are not obvious, especially for researchers who don't have
software development experience.

While there are many different container technologies, this article
focuses on Docker {[}REF{]} as it is the most commonly used {[}REF{]}.
The goal at hand is to write a \texttt{Dockerfile} so that it best
facilitates interactive development and research as well as the higher
goals of reproducibility. A daily commitment to these general practices
can ensure that workflows are reproducible, and generation of a
container is not an afterthought triggered by a publication
(cf.~thoughts on openness as an afterthought by {[}9{]} and on
computational reproducibility by {[}10{]}). By following the
\emph{conventions} layed out in these ten rules, authors ensure
readability by others and ideally subsequent reuse and collaboration.

\hypertarget{docker}{%
\subsection*{Docker}\label{docker}}
\addcontentsline{toc}{subsection}{Docker}

Docker is a container technology that is widely adopted and supported on
many platforms, and has become highly useful in science. They are
distinct from virtual machines (VM), or hypervisors as they don't
emulate hardware, and thus require the same system resources. To build
Docker containers, we write text files that follow a particular format
called \texttt{Dockerfile}s. \texttt{Dockerfile}s are machine- and
human-readable recipes for creating containers. While Docker was the
original technology to support this format, other container technologies
have developed around the format (and thus support it) including:
\href{https://podman.io/}{podman}/\href{https://github.com/containers/buildah}{buildah}
supported by RedHat,
\href{https://github.com/GoogleContainerTools/kaniko}{kaniko},
\href{https://github.com/genuinetools/img}{img}, and
\href{https://github.com/moby/buildkit}{buildkit}. The Singularity
container software {[}11{]} is optimized for high performance computing
and although it uses it's own ``Singularity recipe'' format, it can
import Docker containers directly from a Docker registry. Although the
Singularity recipe format is different, the rules here are transferable
to some extent. While some {[}12{]} can argue for reasons to not publish
reproducibly (lack of time \& incentives and unfittingness for a
researcher's workflow) and there are substantial technical challenges to
maintain software and documentation, providing a \texttt{Dockerfile} or
pre-built Docker container should become an increasingly easier task for
the average researcher. If a researcher is able to find or create
containers or \texttt{Dockerfile}s to address their most common use
cases, then arguably it will not be extra work after this initial
set(cf.~README of {[}13{]}). In fact, the \texttt{Dockerfile} itself can
be used as documentation to clearly show from where data and code was
derived, and consequently where a third party might obtain them again.
Importantly, the researcher should clearly state the platforms (by way
of one or more Docker containers) have been tested on.

\hypertarget{use-tools-to-assist-with-dockerfile-generation.}{%
\section*{1. Use tools to assist with Dockerfile
generation.}\label{use-tools-to-assist-with-dockerfile-generation.}}
\addcontentsline{toc}{section}{1. Use tools to assist with Dockerfile
generation.}

Writing a Dockerfile from scratch is not that simple, and even
``experts'' sometimes take shortcuts. Thus, it's a good strategy to
first look to tools that can help to generate a Dockerfile for you. Such
tools have likely thought about and implemented good practices. It's
always a good practice to start with your specific use case. You would
first want to determine if there is an already existing container base
that you can use, and in this case, use it and add to your software
documentation instructions for doing so. As an example, you might be
doing some kind of interactive development. For interactive development
environments such as notebooks and development servers or databases, you
can readily find many containers that come installed with the software
that you need. In the case that there isn't an existing container for
your needs, you might next look to well-maintained tools to help with
\texttt{Dockerfile} generation. Well maintained means that recent
container bases are used, likely from the Docker library, to ensure that
the container has the most recent security fixes for the operating
system in question. As an example, repo2docker {[}REF repo2docker{]} is
a tool that is maintained by Jupyter Labs that can help to transform a
repository in the format of some known kind of package (notebook,
Python, R, etc.) into a container. As an example, we might install
\texttt{jupyter-repo2docker} and then run it against a repository with a
\texttt{requirements.txt} file, an indication of being a Python package.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{jupyter-repo2docker}\NormalTok{ https://github.com/norvig/pytudes}
\end{Highlighting}
\end{Shaded}

The resulting container image would install the dependencies listed in
the requirements file, along with providing an entrypoint to run a
notebook server to easily interact with any previously existing
notebooks in the container. A precaution that needs to be taken is that
the default command above will create a home for the current user,
meaning that the container itself wouldn't be ideal to share, but rather
any researchers interested in interaction with the code inside should
build their own container. For this reason, it's good practice to look
at any help command provided by the tool and check for configuration
options for user names, user ids, and similar. It's also recommended (if
you are able) to add custom labels to your container build to define
metadata for your analyses, if relevant. The container should be built
to be optimized for its use case, whether it is intended to be shared or
used by a single user. However, in the case that a tool or interactive
container environment is not available, you will likely need to write a
\texttt{Dockerfile} from scratch. In this case, proceed with following
the remaining rules to write your \texttt{Dockerfile}.

\hypertarget{use-versioned-and-automatically-built-base-images}{%
\section*{2. Use versioned and automatically built base
images}\label{use-versioned-and-automatically-built-base-images}}
\addcontentsline{toc}{section}{2. Use versioned and automatically built
base images}

It's good practice to use base images that are maintained by the Docker
library. While some organizations can be trustworthy in terms of
content, and reliable to update containers with security fixes (e.g.,
\texttt{rocker/r-ver}), for most individual accounts that build
containers, it's likely that containers will not be updated regularly. A
good understanding of how base images and tags work is helpful in this
case, as the tag that you choose has implications for your container
build. You should know how to start with a \texttt{Dockerfile}, and use
\texttt{FROM} statements to trace all the way up to the original
container base, an abstract base called \texttt{scratch}. Doing this
kind of trace is essential to be able to be aware of all of the steps
that were taken to generate your container. A tag like \texttt{latest}
is good in that security fixes and updates are likely present, however
in that it's a moving target, it's more likely that an updated base
could break your software. Other tags that should be avoided are
\texttt{dev}, \texttt{devel}, or \texttt{nightly} that may provide
possibly unstable versions of the software. As an example, the Python
container on Docker Hub will be released with new versions of Python. If
you build a container with \texttt{python:latest} that only supports
Python version 3, when the base is updated to Python 4 it's likely that
your software won't work as expected. In this case, a tag like
\texttt{python:3.5} might be a good choice. When you choose a base
image, choose one with a Linux distribution that supports the software
stack you are using, and also take into account the bases that are
widely used by your community. As an example, Ubuntu is heavily used for
geospatial research, and so the \texttt{rocker/geospatial} image would
be a good choice for spatial data science with R, or
\texttt{jupyter/tensorflow-notebook} could be a good choice for machine
learning with Python. Especially when data is involved, containers can
grow quickly in size, and you should keep this in mind. If you need to
built smaller containers, consider a busybox {[}REF{]} base or a
multi-stage build {[}REF{]}. Smaller images often are indicated by
having \texttt{slim} or \texttt{minimal} as part of the tag. Also take
into account the libraries that you need. Base images that have complex
software installed (e.g.~machine learning libraries, specific BLAS
library) are helpful and fine to use. To not rely on a third party or
other individual to maintain the recipe and container, you should copy
the Dockerfile to your own repository, and also provide a deployment for
it via your own automated build. Trusting that the third party
Dockerfile will persist for your analyses is a risky thing to do. For
the new user, a potential list of existing communities with regular
builds and updates includes:

\begin{itemize}
\tightlist
\item
  \href{https://www.rocker-project.org/}{Rocker} for R {[}1{]}
\item
  \href{https://bioconductor.org/help/docker/}{Docker containers for
  Bioconductor} for bioinformatics
\item
  \href{https://hub.docker.com/_/neurodebian}{NeuroDebian images} for
  neuroscience {[}14{]}
\item
  {[}Jupyter Docker
  Stacks{]}(https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html
  for Notebook-based computing
\item
  \href{https://hub.docker.com/r/taverna/taverna-server}{Taverna Server}
  for running Taverna workflows
\end{itemize}

For example, here is how we would use a base image \texttt{r-ver} with
tag \texttt{3.5.2} from the \texttt{rocker} organization on Docker Hub
(\texttt{docker.io})

\begin{verbatim}
FROM rocker/r-ver:3.5.2
\end{verbatim}

\hypertarget{use-formatting-and-favor-clarity}{%
\section{3. Use formatting and favor
clarity}\label{use-formatting-and-favor-clarity}}

\begin{itemize}
\tightlist
\item
  indentation, newlines, and comments are crucial for documentation,
  readability and structure
\item
  Labels {[}REF{]} can provide human and machine readable documentation
  about software
\item
  use comments to provide a guide to others and future you. For example,
  it's helpful to provide commented lines to show how to build and run
  the image. Following a common coding aphorism, we might say \emph{``A
  Dockerfile written three months ago may just as well have been written
  by someone else''}.
\item
  Always add a complete section in a README alongside the Dockerfile for
  exactly how to build, run, and otherwise interact with the container.
  If the container is provided on Docker Hub, you should direct the user
  to it.
\item
  modularisation is a two-edged sword (point out
  \href{https://www.mankier.com/1/podman-build}{\texttt{podman}'s
  \texttt{\#include} directive}?) 
\item
  carefully indent commands and their arguments to make clear what
  belongs together, especially when connecting multiple commands in onr
  \texttt{RUN} with \texttt{\&\&}
\item
  use \texttt{\textbackslash{}} for newlines
\item
  put each dependency on it's own line to make it easier to spot changes
  in version control
\item
  don't let lines get too long
\item
  split up an instruction (especially relevant for \texttt{RUN}) when
  you have to scroll to see all of it
\item
  use a linter to follow common practices and consistency 
\item
  \textbf{Use comments to document decisions and usage}

  \begin{itemize}
  \tightlist
  \item
    make the \texttt{Dockerfile} self-explanatory by adding comments for
    specific decisions
  \item
    add reasons and links to followed tutorials (failed attempts may be
    found in the history)
  \item
    similar to ``literate programming''
  \item
    include commands that did \emph{not} work, especially if they seem
    simpler, so you're not falling into the same trap twice
  \end{itemize}
\end{itemize}

\begin{verbatim}
# apt-get install specific version

# RUN command spreading several lines
RUN install2.r \
  fortunes \
  here

# this library must be installed from source to get version newer than in sources

# following commands from instructions at LINK HERE
\end{verbatim}

\begin{itemize}
\tightlist
\item
  clarity is always more important than brevity
\item
  don't worry about image size, clarity is more important (i.e.~no
  complex RUN instructions that remove files after using rightaway)

  \begin{itemize}
  \tightlist
  \item
    make \texttt{RUN} instructions do one thing (e.g.~install one
    software, install many softwares from \emph{one} source, \ldots{})
  \item
    a single \texttt{RUN} instruction should not be longer then ``one
    page'' \textgreater{} no scrolling
  \end{itemize}
\item
  have commands \emph{in order} of least likely to change to most likely
  to change, it helps readers and takes advantage of build caching

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    system libraries (you can use comments to document which library is
    required by which language-specific package)
  \item
    language-specific libraries or modules
  \item
    from repositories (binaries)
  \item
    from source
  \item
    own software/scripts (if not mounted)
  \item
    labels
  \item
    \texttt{RUN}/\texttt{ENTRYPOINT}
  \end{enumerate}
\item
  how to access the layer commands from an existing image
  (\texttt{docker\ inspect..})
\item
  Only switch directoryies with \texttt{WORKDIR} \{-\}

  \begin{itemize}
  \tightlist
  \item
    might need to move to different directories for bespoke
    configuration or building from source
  \item
    is is much more transparent than \texttt{cd\ X} or \texttt{cd\ ...}
    in \texttt{RUN} statements
  \end{itemize}
\item
  if need be use \emph{layered builds} to only keep specific files from
  one build step to another, e.g.~for build dependencies if building
  software from source
\end{itemize}

\hypertarget{pin-versions}{%
\section*{4. Pin versions}\label{pin-versions}}
\addcontentsline{toc}{section}{4. Pin versions}

\textbf{system libraries}

\begin{itemize}
\tightlist
\item
  you can install specific versions of system packages with the
  respective package manager, also called version pinning

  \begin{itemize}
  \tightlist
  \item
    on apt: https://blog.backslasher.net/my-pinning-guidelines.html
  \end{itemize}
\item
  do so if the version is relevant, e.g.~to demonstrate a bug, or likely
  to become a problem, e.g.~because of \ldots{}
\item
  do so if you are aware of the system library being relevant to your
  workflow
\item
  you can find out about the currently installed versions

  \begin{itemize}
  \tightlist
  \item
    Debian/Ubuntu: \texttt{dpkg\ -\/-list}
  \item
    Alpine: \texttt{apk\ -vv\ info\textbar{}sort}
  \item
    CentOS: \texttt{yum\ list\ installed} or \texttt{rpm\ -qa}
  \end{itemize}
\item
  \emph{installing from source} is a useful way to install very specific
  versions, at the cost of needing build libraries (which could be
  removed again with build stages, but that's advanced)
\end{itemize}

\textbf{extension packages and programming language modules}

\begin{itemize}
\tightlist
\item
  package managers of programming languages are a good solution to
  install a collection of dependencies for a language
\item
  package managers have a CLI and can be used from \texttt{RUN} commands
\item
  freezing the environment,
  cf.~https://markwoodbridge.com/2017/03/05/jupyter-reproducible-science.html
  cited by {[}2{]}
\item
  there is a risk in outsourcing configuration to the file formats
  supported by package managers \textgreater{} use only when direct
  installation in Dockerfile becomes complex; example files:

  \begin{itemize}
  \tightlist
  \item
    Python: \texttt{requirements\ .txt}, \texttt{xxx.yml} (Conda)
  \item
    R: \texttt{DESCRIPTION}
  \item
    Java: \texttt{mvn.xml}
  \item
    JavaScript: \texttt{package.json} of \texttt{npm}
  \end{itemize}
\item
  how to do in Python (\texttt{==\ x.y.z})
\item
  do it in R with \texttt{versions} package, or by using MRAN (e.g.~via
  \texttt{r-ver} image)- JavaScript?
\item
  Julia: \texttt{add\ Package@1.0} \textgreater{}
  https://julialang.github.io/Pkg.jl/v1/managing-packages/\#Adding-packages-1
\item
  Use common command-line ready installation commands of programming
  languages

  \begin{itemize}
  \tightlist
  \item
    better readbiliy, potentially even performance
    (\texttt{RUN\ install2.r\ sp} instead of
    \texttt{RUN\ R\ -e\ "install.packages(\textquotesingle{}sf\textquotesingle{})"},
    although the latter is ``base R'')
  \end{itemize}
\end{itemize}

\hypertarget{mount-data-and-control-code}{%
\section*{5. Mount data and control
code}\label{mount-data-and-control-code}}
\addcontentsline{toc}{section}{5. Mount data and control code}

The addition of data or code into the container using
\texttt{ADD}/\texttt{COPY} is highly dependent on your use case. For
example, small data files and software that is essential for the
function of a container to distribute a reproducible analysis is
essential to add. However, if the container is intended to be used as an
interactive environment, you should refain from adding it to the
container, and instead bind the files from the outside of the container.
This also ensures that data persists when the container instance or
image is removed from your system. When you do this, in the case that
you will likely save as a root user from inside the container, you
should be careful about file permissions. To avoid problems when
mounting, Docker has provided a \texttt{-\/-user} option that you should
use. Without this option, there can be unexpected issues with
permissions. For example, writing a new file from inside the container
will be owned by user ``root'' on your host. In that many applications
generate log and other output files during runtime, be cautious about
what volumes are bound to the host.

The other logical case to bind mount code is when your data is very
large (usually over hundreds of GB) or has limited access. For example,
a private dataset with personal health information (PHI) should never be
added to a container. In these cases, you should provide clear
instructions to the user in the README for how to obtain actual or dummy
data.

\begin{itemize}
\tightlist
\item
  easier access, does not require Docker knowledge by third parties to
  reause code and data
\item
  if you have a ``stable'' published software library, install it from
  source from the source code repo or from the software repository (so
  that users find the project in the future)
\item
  how to mount the data can be put in the example commands (see 4.)
\item
  prefer the long versions of CLI parameters for readability
\end{itemize}

\begin{verbatim}
docker run --volume ...
\end{verbatim}

\hypertarget{capture-environment-metadata}{%
\section*{6. Capture environment
metadata}\label{capture-environment-metadata}}
\addcontentsline{toc}{section}{6. Capture environment metadata}

Use labels and build arguments for metadata

\begin{itemize}
\tightlist
\item
  \textbf{labels}

  \begin{itemize}
  \tightlist
  \item
    advantage of labels: are structured metadata, can be exposed by
    APIs, e.g.~https://microbadger.com/labels
  \item
    use namespaced-names in labels (http://label-schema.org/rc1/
    respectively https://github.com/opencontainers/image-spec)

    \begin{itemize}
    \tightlist
    \item
      should we use the article to establish some core metadata fields
      for research? author DOI? research organisations
      (https://ror.org/)? funding agency/grant number?
    \end{itemize}
  \item
    are part of the exported Docker image, mention
    \texttt{docker\ inspect}
  \item
    important metadata items

    \begin{itemize}
    \tightlist
    \item
      repository link where Dockerfile is published
    \item
      author (\texttt{MAINTAINER} is deprecated) and contact
      (e.g.~email, project website)
    \item
      license
    \item
      usage instructions?
    \item
      DOI of research compendium (Zenodo preregister instead of GitHub
      automatic integration?)
    \end{itemize}
  \item
    can https://codemeta.github.io/ and
    https://citation-file-format.github.io/ be used/useful?
  \end{itemize}
\item
  \textbf{build arguments}

  \begin{itemize}
  \tightlist
  \item
    use build arguments to capture build metadata
  \item
    git commit hash
  \item
    date and timestamp
  \item
    clarifies if build was automated
  \end{itemize}
\end{itemize}

\hypertarget{enable-interactive-usage-and-one-click-execution}{%
\section*{7. Enable interactive usage and one-click
execution}\label{enable-interactive-usage-and-one-click-execution}}
\addcontentsline{toc}{section}{7. Enable interactive usage and one-click
execution}

\begin{itemize}
\tightlist
\item
  using \texttt{CMD} and \texttt{ENTRYPOINT} make sure that it is
  possible to run the container interactively \emph{and} as a one-click
  execution \textgreater{} give examples (see below)
\item
  the default execution should either execute the workflow (headless) or
  start an analysis environment

  \begin{itemize}
  \tightlist
  \item
    if your workflow/sofware does not support headless execution
    (Excel?), switch tools
  \item
    or have default with UI and only document headless execution via
    example commands
  \end{itemize}
\item
  may also use the same \texttt{Dockerfile} for different purposes,
  e.g.~include an app (e.g.~Shiny) for interactive use by user
\item
  if you want to expose a user interface \textbf{use the browser} on and
  exposed port, unless you're using an existing Desktop you, then you
  can use \texttt{x11docker} {[}{]}
\item
  one useful alternative: Notebook user interfaces in the browser
  (Jupyter, RStudio)
\item
  document both variants with example commands
\item
  a headless execution can be executed in a continuous integration (CI)
  after every project update, potentially on a test dataset for
  speed-up,
\item
  see also Rule 7: Build a pipeline in {[}2{]}
\item
  you can also make your workflow configurable, e.g.~by bespoke
  configuration files, environment variables passed to the container
  {[}15{]}, or special Docker-based wrappers such as Kliko {[}16{]};
  however, know this is a trade-off from plain \texttt{Dockerfile}-based
  solutions, which is a proven industry standard
\item
  \emph{what user should run within the Dockerfile?}
\end{itemize}

\hypertarget{end-the-dockerfile-with-build-and-run-commands}{%
\section{\texorpdfstring{8. End the \texttt{Dockerfile} with build and
run
commands}{8. End the Dockerfile with build and run commands}}\label{end-the-dockerfile-with-build-and-run-commands}}

\begin{itemize}
\tightlist
\item
  put \texttt{docker\ run} and \texttt{docker\ build} commands in
  comments at the end of the file (\emph{may be own rule?}), especially
  relevant if arguments are used such as port exposure or mount points
\item
  should be copy-and-pasteable
\item
  use multiple comment characters to make clear what is command and what
  is documentation
\end{itemize}

\begin{verbatim}
# Build the images with
##> docker build --tag great_workflow .
# Run the image:
##> docker run --it --port 80:80 --volume ./input:/input --name gwf great_workflow
# Extract the data:
##> docker cp gwf:/output/ ./output
\end{verbatim}

\hypertarget{publish-a-dockerfile-per-project-in-a-code-repository-with-version-control}{%
\section*{9. Publish a Dockerfile per project in a code repository with
version
control}\label{publish-a-dockerfile-per-project-in-a-code-repository-with-version-control}}
\addcontentsline{toc}{section}{9. Publish a Dockerfile per project in a
code repository with version control}

\begin{itemize}
\tightlist
\item
  \texttt{Dockerfile} is a plain text-based format and therefore you
  should put it under version control
\item
  just as code, the \texttt{Dockerfile} should be for humans to
  understand and just incidentally for machines to be interpretable
  (qoute Abelson? see also
  https://www.quora.com/How-true-is-Programs-are-meant-to-be-read-by-humans-and-only-incidentally-for-computers-to-execute)
\item
  using and publishing a \texttt{Dockerfile} to create a container will
  increase chances of preservation (cf. {[}17{]})
\item
  Consult Ten Simple Rules paper by Perez-Riverol et al. {[}18{]}
\item
  add the link to the online repository to a label, to point back to the
  source of the file
\item
  versioning on a collaboration platform exposes your environment
  configuration and enables collaboration/feedback
\item
  you can build and run (e.g.~on a test dataset!) you Dockerfile in CI
  (cf.~automation below)
\item
  keep \texttt{Dockerfile} in the same project with your workflow and
  data (cf.~research compendium concept?)
\item
  \textbf{this should be the repository with the workflow and data}
  (cf.~research compendium)

  \begin{itemize}
  \tightlist
  \item
    Use one \texttt{Dockerfile} per workflow or project and put one
    ``thing'' in; \textbf{TO DISCUSS}: argue against the above rule and
    recommend having a process manager and multiple processes in one
    container
  \item
    start with a clean slate for a new project - shared lines are
    quickly copied over, and Docker's build caching will bring some
    performance
  \item
    allows you to quickly switch between projects and not worry about
    breaking things you are not working on
  \item
    have one obvious main process per project, e.g. \texttt{R} or
    RStudio
  \item
    if you have a complex set-up of several proecceses, e.g.~with a
    database, then put it in a separate container and connect them via
    \texttt{docker-compose}
  \end{itemize}
\item
  use git commit messages extensively to describe the reasons behind
  changes; the messages may even contain failed attempts/commands
\item
  add a clear license
\item
  publish the image of the workflow to a suitable repository (where it
  gets a DOI) at the time of publication of the workflow
\item
  you may use multiple containers and \texttt{Dockerfiles} for complex
  workflows (cf. {[}19{]}) but then you're probably out of scope of this
  article
\end{itemize}

\hypertarget{use-the-container-daily-rebuild-the-image-weekly}{%
\section*{10. Use the container daily, rebuild the image
weekly}\label{use-the-container-daily-rebuild-the-image-weekly}}
\addcontentsline{toc}{section}{10. Use the container daily, rebuild the
image weekly}

\begin{itemize}
\tightlist
\item
  use the container built by the \texttt{Dockerfile} in your regular
  work, it is the only way to make sure it is really stable
  (cf.~Marwick's ``this container is the only way I have ever run this
  workflow'')
\item
  no showstopper for using UIs (web-based, e.g.~Jupyter, RStudio, but
  also \texttt{x11docker})
\item
  you cannot expect to take a year old Docker image form the shelf and
  that can be extended, it will likely ``run'' but just as-is
  \textgreater{} need to re-build ``all the time'' to stay reusable; the
  longer you wait with trying to recompile the image the harder it will
  get (you don't know for which of the different reasons it fails)
\item
  during development and analysis, interactive use (e.g.~R session,
  Jupyter Notebook) has advantages, and even the most disciplined might
  install a package or change a parameter manually
\item
  regularly delete all containers and rebuild images based on your
  \texttt{Dockerfile}
\item
  you are more likely to remember the undocumented steps if done
  regularly
\item
  increases trust in configuration, encourages effetiveness and fully
  scripted configuration
\item
  keep a \texttt{Makefile} next to the Dockerfile so you don't fall into
  the trap of not regularly rebuilding your digital laboratory (better
  to have build and run commands - i.e.~the usage - in two places and
  potentially diverging than the actual \texttt{Dockerfile})
\item
  \textbf{Don't replicate environment configuration outside of the
  Dockerfile for convenience}

  \begin{itemize}
  \tightlist
  \item
    make the Dockerfile work for your day-to-day research instead of
    having a second set of configurations in on the ``local'' machine
  \item
    having two approaches will eventually break, only a perceived
    convenience
  \item
    avoid an untidy laboratory in practice behind a shiny appearance of
    a \texttt{Dockerfile}
  \item
    you can install interactive UIs as part of the Dockerfile and use
    them just like Desktop UIs (Jupyter, RStudio, use )
  \end{itemize}
\item
  if you can, get a colleague to run the workflow for you, or even
  better switch Dockerfiles and give feedback - this gives both of you
  an extra layer of confidence
\item
  add a CRON job that deletes the image every week?
\end{itemize}

\textbf{Box: Automatic generation of Dockerfiles}

\begin{itemize}
\tightlist
\item
  there are tools you can auto-generate a \texttt{Dockerfile}
\item
  can be a good as a starting point, careful to avoid a lock-in
\item
  they have limitations, namely \ldots{}
\item
  \texttt{repo2docker}, \texttt{dockter}, \texttt{containerit}
\item
  these are useful if you don't need very specific versions etc. and for
  specific use cases, but sometimes requires a specific project
  structure (PyPI \texttt{requirements.txt}) or reproducible document (R
  Markdown file)
\end{itemize}

\hypertarget{example-dockerfiles}{%
\section{Example Dockerfiles}\label{example-dockerfiles}}

To demonstrate the 10 rules, we have a git repository with example
\texttt{Dockerfile}s, some of which we took from public repositories and
updated to adhere to the rules (\texttt{Dockerfile.before} and
\texttt{Dockerfile.after}).

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

Reproducibility in research is an endeavour of best efforts, not about
achieving the perfect solution, as that is probably not achievable or
changing over time. This article provides guidance for using
\texttt{Dockerfile}s in computational/computer-based research to work
towards a ``time capsule'' (see
https://twitter.com/DougBlank/status/1135904909663068165?s=09) that
given some expertise and the right tools can be used to come as close as
possible to the original virtual laboratory used for a specific. Such an
increase in transparency and conscious effort is valuable for the
creators of analyses, even if the capsule decays over time. The effort
should also be valued by others and may change the way scholars
collaborate and communicate (cf.~notion of ``preproducibility'' by
{[}20{]}) So please, don't go insane with writing \texttt{Dockerfile}s,
but be realistic about what might break and what is unlikely to break.
In a similar vein, these rules may be broken if another way works better
for \emph{your use case}. The rules in this article help you mastering
the \texttt{Dockerfile} format for research and provide a solid basis
for engaging in more complex but also in simpler assisted usage of
containers (cf.~Box: Aumatic Generation). Corner cases aside, share and
exchange your \texttt{Dockerfile} freely and collaborate in your
community to spread the knowledge about containers as a tool for
research. Together you can develop common practices or even shared base
images (exemplified by communities listed in Rule 1).

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

o2r by DFG

\hypertarget{contributions}{%
\section*{Contributions}\label{contributions}}
\addcontentsline{toc}{section}{Contributions}

DN conceived the idea, wrote the first darft, contributed to all rules.
SE contributed to the overall structure and selected rules. TH
contributed to the rule structure and particularly rule 1.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-boettiger_introduction_2017}{}%
1. Boettiger C, Eddelbuettel D. An Introduction to Rocker: Docker
Containers for R. The R Journal. 2017;9: 527--536.
doi:\href{https://doi.org/10.32614/RJ-2017-065}{10.32614/RJ-2017-065}

\leavevmode\hypertarget{ref-rule_ten_2019}{}%
2. Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et
al. Ten simple rules for writing and sharing computational analyses in
Jupyter Notebooks. PLOS Computational Biology. 2019;15: e1007007.
doi:\href{https://doi.org/10.1371/journal.pcbi.1007007}{10.1371/journal.pcbi.1007007}

\leavevmode\hypertarget{ref-sandve_ten_2013}{}%
3. Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for
Reproducible Computational Research. PLoS Comput Biol. 2013;9: e1003285.
doi:\href{https://doi.org/10.1371/journal.pcbi.1003285}{10.1371/journal.pcbi.1003285}

\leavevmode\hypertarget{ref-brinckman_computing_2018}{}%
4. Brinckman A, Chard K, Gaffney N, Hategan M, Jones MB, Kowalik K, et
al. Computing environments for reproducibility: Capturing the ``Whole
Tale''. Future Generation Computer Systems. 2018;
doi:\href{https://doi.org/10.1016/j.future.2017.12.029}{10.1016/j.future.2017.12.029}

\leavevmode\hypertarget{ref-code_ocean_2019}{}%
5. Code Ocean {[}Internet{]}. 2019. Available:
\url{https://codeocean.com/}

\leavevmode\hypertarget{ref-simko_reana_2019}{}%
6. Šimko T, Heinrich L, Hirvonsalo H, Kousidis D, Rodríguez D. REANA: A
System for Reusable Research Data Analyses. EPJ Web of Conferences.
2019;214: 06034.
doi:\href{https://doi.org/10.1051/epjconf/201921406034}{10.1051/epjconf/201921406034}

\leavevmode\hypertarget{ref-jupyter_binder_2018}{}%
7. Jupyter P, Bussonnier M, Forde J, Freeman J, Granger B, Head T, et
al. Binder 2.0 - Reproducible, interactive, sharable environments for
science at scale. Proceedings of the 17th Python in Science Conference.
2018; 113--120.
doi:\href{https://doi.org/10.25080/Majora-4af1f417-011}{10.25080/Majora-4af1f417-011}

\leavevmode\hypertarget{ref-nust_opening_2017}{}%
8. Nüst D, Konkol M, Pebesma E, Kray C, Schutzeichel M, Przibytzin H, et
al. Opening the Publication Process with Executable Research Compendia.
D-Lib Magazine. 2017;23.
doi:\href{https://doi.org/10.1045/january2017-nuest}{10.1045/january2017-nuest}

\leavevmode\hypertarget{ref-chen_open_2019}{}%
9. Chen X, Dallmeier-Tiessen S, Dasler R, Feger S, Fokianos P, Gonzalez
JB, et al. Open is not enough. Nature Physics. 2019;15: 113.
doi:\href{https://doi.org/10.1038/s41567-018-0342-2}{10.1038/s41567-018-0342-2}

\leavevmode\hypertarget{ref-donoho_invitation_2010}{}%
10. Donoho DL. An invitation to reproducible computational research.
Biostatistics. 2010;11: 385--388.
doi:\href{https://doi.org/10.1093/biostatistics/kxq028}{10.1093/biostatistics/kxq028}

\leavevmode\hypertarget{ref-kurtzer_singularity_2017}{}%
11. Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers
for mobility of compute. PLOS ONE. 2017;12: e0177459.
doi:\href{https://doi.org/10.1371/journal.pone.0177459}{10.1371/journal.pone.0177459}

\leavevmode\hypertarget{ref-boettiger_introduction_2015}{}%
12. Boettiger C. An Introduction to Docker for Reproducible Research.
SIGOPS Oper Syst Rev. 2015;49: 71--79.
doi:\href{https://doi.org/10.1145/2723872.2723882}{10.1145/2723872.2723882}

\leavevmode\hypertarget{ref-marwick_madjebebe_2015}{}%
13. Ben Marwick. 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-halchenko_open_2012}{}%
14. Halchenko YO, Hanke M. Open is Not Enough. Let's Take the Next Step:
An Integrated, Community-Driven Computing Platform for Neuroscience.
Frontiers in Neuroinformatics. 2012;6.
doi:\href{https://doi.org/10.3389/fninf.2012.00022}{10.3389/fninf.2012.00022}

\leavevmode\hypertarget{ref-knoth_reproducibility_2017}{}%
15. Knoth C, Nüst D. Reproducibility and Practical Adoption of GEOBIA
with Open-Source Software in Docker Containers. Remote Sensing. 2017;9:
290. doi:\href{https://doi.org/10.3390/rs9030290}{10.3390/rs9030290}

\leavevmode\hypertarget{ref-molenaar_klikoscientific_2018}{}%
16. Molenaar G, Makhathini S, Girard JN, Smirnov O. Kliko---The
scientific compute container format. Astronomy and Computing. 2018;25:
1--9.
doi:\href{https://doi.org/10.1016/j.ascom.2018.08.003}{10.1016/j.ascom.2018.08.003}

\leavevmode\hypertarget{ref-emsley_framework_2018}{}%
17. Emsley I, De Roure D. A Framework for the Preservation of a Docker
Container International Journal of Digital Curation. International
Journal of Digital Curation. 2018;12.
doi:\href{https://doi.org/10.2218/ijdc.v12i2.509}{10.2218/ijdc.v12i2.509}

\leavevmode\hypertarget{ref-perez-riverol_ten_2016}{}%
18. Perez-Riverol Y, Gatto L, Wang R, Sachsenberg T, Uszkoreit J,
Leprevost F da V, et al. Ten Simple Rules for Taking Advantage of Git
and GitHub. PLOS Computational Biology. 2016;12: e1004947.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004947}{10.1371/journal.pcbi.1004947}

\leavevmode\hypertarget{ref-kim_bio-docklets_2017}{}%
19. Kim B, Ali TA, Lijeron C, Afgan E, Krampis K. Bio-Docklets:
Virtualization Containers for Single-Step Execution of NGS Pipelines.
bioRxiv. 2017; 116962.
doi:\href{https://doi.org/10.1101/116962}{10.1101/116962}

\leavevmode\hypertarget{ref-stark_before_2018}{}%
20. Stark PB. Before reproducibility must come preproducibility
{[}Internet{]}. Nature. 2018.
doi:\href{https://doi.org/10.1038/d41586-018-05256-0}{10.1038/d41586-018-05256-0}

\nolinenumbers


\end{document}

